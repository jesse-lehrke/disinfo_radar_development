{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usuals\n",
    "import numpy as np\n",
    "from numpy import quantile, where, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "# Scientific\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils as skl_utils\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Supporting\n",
    "\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = '../data/'\n",
    "OUTPUT = '../output_data/'\n",
    "MODEL_PATH = '../data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index, Filename\n",
      "[(0, 'deepfake_txt.csv'), (1, 'arxiv_disinformation.csv'), (2, 'arxiv_deepfake.csv'), (3, 'results.csv'), (4, 'reddit_machinelearning.csv')]\n"
     ]
    }
   ],
   "source": [
    "# Check files in data folder\n",
    "datafiles = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f))]\n",
    "\n",
    "print('Index, Filename')\n",
    "print(list(zip([index for index, value in enumerate(datafiles)], datafiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a file name, can use\n",
    "filename = datafiles[3]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "CONVERTERS = {'tokens': eval, 'published_parsed': eval, 'tags': eval, 'arxiv_primary_category': eval}\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + filename, converters=CONVERTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>arxiv_comment</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>search_term</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>category</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2204.02960v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2204.02960v1</td>\n",
       "      <td>2022-04-06T17:54:46Z</td>\n",
       "      <td>[2022, 4, 6, 17, 54, 46, 2, 96, 0]</td>\n",
       "      <td>2022-04-06T17:54:46Z</td>\n",
       "      <td>[2022, 4, 6, 17, 54, 46, 2, 96, 0]</td>\n",
       "      <td>Simple and Effective Synthesis of Indoor 3D Sc...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>We study the problem of synthesizing immersive...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAN</td>\n",
       "      <td>we study the problem of synthesizing immersive...</td>\n",
       "      <td>[study, problem, synthesize, immersive, indoor...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2204.02591v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2204.02591v1</td>\n",
       "      <td>2022-04-06T05:51:04Z</td>\n",
       "      <td>[2022, 4, 6, 5, 51, 4, 2, 96, 0]</td>\n",
       "      <td>2022-04-06T05:51:04Z</td>\n",
       "      <td>[2022, 4, 6, 5, 51, 4, 2, 96, 0]</td>\n",
       "      <td>Contextual Attention Mechanism, SRGAN Based In...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>The new alternative is to use deep learning to...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAN, fake news</td>\n",
       "      <td>the new alternative is to use deep learning to...</td>\n",
       "      <td>[new, alternative, use, deep, learning, inpain...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2204.02411v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2204.02411v1</td>\n",
       "      <td>2022-04-05T18:00:04Z</td>\n",
       "      <td>[2022, 4, 5, 18, 0, 4, 1, 95, 0]</td>\n",
       "      <td>2022-04-05T18:00:04Z</td>\n",
       "      <td>[2022, 4, 5, 18, 0, 4, 1, 95, 0]</td>\n",
       "      <td>Texturify: Generating Textures on 3D Shape Sur...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Texture cues on 3D objects are key to compelli...</td>\n",
       "      <td>...</td>\n",
       "      <td>Project Page: https://nihalsid.github.io/textu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAN</td>\n",
       "      <td>texture cues on 3d objects are key to compelli...</td>\n",
       "      <td>[texture, cue, object, key, compelling, visual...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  guidislink  \\\n",
       "0  http://arxiv.org/abs/2204.02960v1        True   \n",
       "1  http://arxiv.org/abs/2204.02591v1        True   \n",
       "2  http://arxiv.org/abs/2204.02411v1        True   \n",
       "\n",
       "                                link               updated  \\\n",
       "0  http://arxiv.org/abs/2204.02960v1  2022-04-06T17:54:46Z   \n",
       "1  http://arxiv.org/abs/2204.02591v1  2022-04-06T05:51:04Z   \n",
       "2  http://arxiv.org/abs/2204.02411v1  2022-04-05T18:00:04Z   \n",
       "\n",
       "                       updated_parsed             published  \\\n",
       "0  [2022, 4, 6, 17, 54, 46, 2, 96, 0]  2022-04-06T17:54:46Z   \n",
       "1    [2022, 4, 6, 5, 51, 4, 2, 96, 0]  2022-04-06T05:51:04Z   \n",
       "2    [2022, 4, 5, 18, 0, 4, 1, 95, 0]  2022-04-05T18:00:04Z   \n",
       "\n",
       "                     published_parsed  \\\n",
       "0  [2022, 4, 6, 17, 54, 46, 2, 96, 0]   \n",
       "1    [2022, 4, 6, 5, 51, 4, 2, 96, 0]   \n",
       "2    [2022, 4, 5, 18, 0, 4, 1, 95, 0]   \n",
       "\n",
       "                                               title  \\\n",
       "0  Simple and Effective Synthesis of Indoor 3D Sc...   \n",
       "1  Contextual Attention Mechanism, SRGAN Based In...   \n",
       "2  Texturify: Generating Textures on 3D Shape Sur...   \n",
       "\n",
       "                                        title_detail  \\\n",
       "0  {'type': 'text/plain', 'language': None, 'base...   \n",
       "1  {'type': 'text/plain', 'language': None, 'base...   \n",
       "2  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                             summary  ...  \\\n",
       "0  We study the problem of synthesizing immersive...  ...   \n",
       "1  The new alternative is to use deep learning to...  ...   \n",
       "2  Texture cues on 3D objects are key to compelli...  ...   \n",
       "\n",
       "                                       arxiv_comment arxiv_doi  \\\n",
       "0                                                NaN       NaN   \n",
       "1                                                NaN       NaN   \n",
       "2  Project Page: https://nihalsid.github.io/textu...       NaN   \n",
       "\n",
       "  arxiv_journal_ref arxiv_affiliation     search_term  \\\n",
       "0               NaN               NaN             GAN   \n",
       "1               NaN               NaN  GAN, fake news   \n",
       "2               NaN               NaN             GAN   \n",
       "\n",
       "                                            cleaning  \\\n",
       "0  we study the problem of synthesizing immersive...   \n",
       "1  the new alternative is to use deep learning to...   \n",
       "2  texture cues on 3d objects are key to compelli...   \n",
       "\n",
       "                                              tokens category  year month_year  \n",
       "0  [study, problem, synthesize, immersive, indoor...    cs.CV  2022     2022-4  \n",
       "1  [new, alternative, use, deep, learning, inpain...    cs.CV  2022     2022-4  \n",
       "2  [texture, cue, object, key, compelling, visual...    cs.CV  2022     2022-4  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data frame\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here for tests we will load a second df, not do a traditional train test split, as we want some sort of bias in the second set - to ensure outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a sub df with select values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake news                986\n",
       "GAN                      984\n",
       "disinformation           222\n",
       "GPT-3                    134\n",
       "GAN, fake news            14\n",
       "GAN, disinformation        1\n",
       "GAN, GPT-3                 1\n",
       "GPT-3, disinformation      1\n",
       "Name: search_term, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.search_term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_list = ['disinformation', 'GAN']\n",
    "df = df.loc[df['search_term'].isin(keep_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAN               984\n",
       "disinformation    222\n",
       "Name: search_term, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.search_term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH + 'test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If \"cleaning\" column kept from Preprocssing, can use that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(txt):\n",
    "        x = ' '.join(txt)\n",
    "        #x = [token.split('/')[0] for token in x] # use when we need lists with just these!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_merged'] = df['tokens'].dropna().apply(lambda x: join_tokens(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2 Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates doc2vec vectors for each document in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, action_column, vector_size=100, learning_rate=0.02, epochs=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "        self.action_column = action_column\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        tagged_x = [TaggedDocument(str(row[self.action_column]).split(), [index]) for index, row in df_x.iterrows()] # edit this: will not work on Chinese\n",
    "\n",
    "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers) # maybe want to try Word2Vec\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train(skl_utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
    "            model.alpha -= self.learning_rate\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        self._model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return np.asmatrix(np.array([self._model.infer_vector(str(row[self.action_column]).split())\n",
    "                                     for index, row in df_x.iterrows()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1206/1206 [00:00<00:00, 4949442.88it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2511584.22it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2852978.36it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2491788.48it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2622255.38it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2652506.88it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2580780.93it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 3760840.61it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2918828.98it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2913784.92it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2928969.67it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2908758.27it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2861046.73it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2970246.99it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2963286.83it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2984265.85it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2788495.38it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 1780475.40it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2063782.38it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2825883.03it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2706436.93it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2517835.05it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2602021.93it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 5392676.57it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2684888.87it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 3660152.41it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2594015.70it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2304478.64it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2915464.34it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2869160.88it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2771688.01it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2861046.73it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2651116.68it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2713696.69it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2797749.24it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 4293998.83it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2693466.79it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2898756.80it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2885528.02it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2885528.02it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2535504.07it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2937474.23it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2666489.52it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2854588.39it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 1940287.93it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2872419.43it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 5048234.16it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2869160.88it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2570289.95it/s]\n",
      "100%|██████████| 1206/1206 [00:00<00:00, 2582098.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initializing model\n",
    "doc2vec_tr = Doc2VecTransformer('tokens_merged', \n",
    "                              vector_size=150,#normally imo 150\n",
    "                              epochs= 50,\n",
    "                              )\n",
    "\n",
    "# Fitting\n",
    "#doc2vec_tr.fit(df)\n",
    "fitted = doc2vec_tr.fit(df)\n",
    "\n",
    "#Transforming\n",
    "doc2vec_vectors = fitted.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2343"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_vectors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving as: ../data/models/arxiv_deepfake_doc_vectors.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/models/arxiv_deepfake_doc_vectors.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADD SAVE KV\n",
    "# ADD LOAD KV\n",
    "m = MODEL_PATH + filename.split('.')[0] + '_doc_vectors.pkl'\n",
    "print('Saving as: ' + m)\n",
    "\n",
    "joblib.dump(fitted, m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index, Model Name\n",
      "[(0, 'arxiv_disinformation_doc_vectors.pkl'), (1, 'arxiv_deepfake_doc_vectors.pkl'), (2, 'train_arxiv_deepfake_doc_vectors.pkl'), (3, 'arxiv_deepfake_svm_model.pkl'), (4, 'arxiv_deepfake_iso_model.pkl')]\n"
     ]
    }
   ],
   "source": [
    "# Check files in models folder\n",
    "models = [f for f in listdir(MODEL_PATH) if isfile(join(MODEL_PATH, f))]\n",
    "\n",
    "print('Index, Model Name')\n",
    "print(list(zip([index for index, value in enumerate(models)], models)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load a tfidf model / just for testing here\n",
    "model_name = datafiles[0]\n",
    "\n",
    "doc2vec_vectors = joblib.load(MODEL_PATH + model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then transform again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Sync/2022_SABRINA_CLIMATE/sab22_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit model\n",
    "'''\n",
    "Adjust nu hyperparameter to, simplifing, \n",
    "increase/decrease \"novelty\" sensitivity. \n",
    "It is very high now = less outliers\n",
    "'''\n",
    "\n",
    "model = OneClassSVM(kernel = 'rbf', \n",
    "                  gamma = 'scale', \n",
    "                  nu = 0.001).fit(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/arxiv_deepfake_svm_model.pkl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, MODEL_PATH + filename.split('.')[0] + \"_svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Sync/2022_SABRINA_CLIMATE/sab22_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_pred = model.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 18\n"
     ]
    }
   ],
   "source": [
    "# Filter outlier index\n",
    "outlier_index = where(y_pred == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)))\n",
    "\n",
    "# Un-used, for inspection\n",
    "#outlier_values = doc2vec_vectors.iloc[outlier_index]\n",
    "#outlier_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with just outliers\n",
    "\n",
    "df_misclass = df[df.index.isin(indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>search_term</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>category</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>tokens_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>http://arxiv.org/abs/2203.14814v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.14814v1</td>\n",
       "      <td>2022-03-28T14:51:42Z</td>\n",
       "      <td>[2022, 3, 28, 14, 51, 42, 0, 87, 0]</td>\n",
       "      <td>2022-03-28T14:51:42Z</td>\n",
       "      <td>[2022, 3, 28, 14, 51, 42, 0, 87, 0]</td>\n",
       "      <td>Stochastic Parameterizations: Better Modelling...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>The modelling of small-scale processes is a ma...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAN</td>\n",
       "      <td>the modelling of smallscale processes is a maj...</td>\n",
       "      <td>[modelling, smallscale, process, major, source...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-3</td>\n",
       "      <td>modelling smallscale process major source erro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>http://arxiv.org/abs/2202.08143v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2202.08143v1</td>\n",
       "      <td>2022-02-16T15:34:09Z</td>\n",
       "      <td>[2022, 2, 16, 15, 34, 9, 2, 47, 0]</td>\n",
       "      <td>2022-02-16T15:34:09Z</td>\n",
       "      <td>[2022, 2, 16, 15, 34, 9, 2, 47, 0]</td>\n",
       "      <td>Bias in Automated Image Colorization: Metrics ...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>We measure the color shifts present in coloriz...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAN</td>\n",
       "      <td>we measure the color shifts present in coloriz...</td>\n",
       "      <td>[measure, color, shift, present, colorized, im...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-2</td>\n",
       "      <td>measure color shift present colorized image da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>http://arxiv.org/abs/2201.10130v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2201.10130v1</td>\n",
       "      <td>2022-01-25T07:06:43Z</td>\n",
       "      <td>[2022, 1, 25, 7, 6, 43, 1, 25, 0]</td>\n",
       "      <td>2022-01-25T07:06:43Z</td>\n",
       "      <td>[2022, 1, 25, 7, 6, 43, 1, 25, 0]</td>\n",
       "      <td>Improving Adversarial Waveform Generation base...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Adversarial waveform generation has been a pop...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAN</td>\n",
       "      <td>adversarial waveform generation has been a pop...</td>\n",
       "      <td>[adversarial, waveform, generation, popular, a...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-1</td>\n",
       "      <td>adversarial waveform generation popular approa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  guidislink  \\\n",
       "47   http://arxiv.org/abs/2203.14814v1        True   \n",
       "240  http://arxiv.org/abs/2202.08143v1        True   \n",
       "331  http://arxiv.org/abs/2201.10130v1        True   \n",
       "\n",
       "                                  link               updated  \\\n",
       "47   http://arxiv.org/abs/2203.14814v1  2022-03-28T14:51:42Z   \n",
       "240  http://arxiv.org/abs/2202.08143v1  2022-02-16T15:34:09Z   \n",
       "331  http://arxiv.org/abs/2201.10130v1  2022-01-25T07:06:43Z   \n",
       "\n",
       "                          updated_parsed             published  \\\n",
       "47   [2022, 3, 28, 14, 51, 42, 0, 87, 0]  2022-03-28T14:51:42Z   \n",
       "240   [2022, 2, 16, 15, 34, 9, 2, 47, 0]  2022-02-16T15:34:09Z   \n",
       "331    [2022, 1, 25, 7, 6, 43, 1, 25, 0]  2022-01-25T07:06:43Z   \n",
       "\n",
       "                        published_parsed  \\\n",
       "47   [2022, 3, 28, 14, 51, 42, 0, 87, 0]   \n",
       "240   [2022, 2, 16, 15, 34, 9, 2, 47, 0]   \n",
       "331    [2022, 1, 25, 7, 6, 43, 1, 25, 0]   \n",
       "\n",
       "                                                 title  \\\n",
       "47   Stochastic Parameterizations: Better Modelling...   \n",
       "240  Bias in Automated Image Colorization: Metrics ...   \n",
       "331  Improving Adversarial Waveform Generation base...   \n",
       "\n",
       "                                          title_detail  \\\n",
       "47   {'type': 'text/plain', 'language': None, 'base...   \n",
       "240  {'type': 'text/plain', 'language': None, 'base...   \n",
       "331  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                               summary  ... arxiv_doi  \\\n",
       "47   The modelling of small-scale processes is a ma...  ...       NaN   \n",
       "240  We measure the color shifts present in coloriz...  ...       NaN   \n",
       "331  Adversarial waveform generation has been a pop...  ...       NaN   \n",
       "\n",
       "    arxiv_journal_ref arxiv_affiliation search_term  \\\n",
       "47                NaN               NaN         GAN   \n",
       "240               NaN               NaN         GAN   \n",
       "331               NaN               NaN         GAN   \n",
       "\n",
       "                                              cleaning  \\\n",
       "47   the modelling of smallscale processes is a maj...   \n",
       "240  we measure the color shifts present in coloriz...   \n",
       "331  adversarial waveform generation has been a pop...   \n",
       "\n",
       "                                                tokens category  year  \\\n",
       "47   [modelling, smallscale, process, major, source...    cs.LG  2022   \n",
       "240  [measure, color, shift, present, colorized, im...    cs.CV  2022   \n",
       "331  [adversarial, waveform, generation, popular, a...    cs.SD  2022   \n",
       "\n",
       "    month_year                                      tokens_merged  \n",
       "47      2022-3  modelling smallscale process major source erro...  \n",
       "240     2022-2  measure color shift present colorized image da...  \n",
       "331     2022-1  adversarial waveform generation popular approa...  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect dataframe\n",
    "\n",
    "df_misclass.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'guidislink', 'link', 'updated', 'updated_parsed', 'published',\n",
       "       'published_parsed', 'title', 'title_detail', 'summary',\n",
       "       'summary_detail', 'authors', 'author_detail', 'author', 'links',\n",
       "       'arxiv_primary_category', 'tags', 'arxiv_comment', 'arxiv_doi',\n",
       "       'arxiv_journal_ref', 'arxiv_affiliation', 'search_term', 'cleaning',\n",
       "       'tokens', 'category', 'year', 'month_year', 'tokens_merged'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_misclass.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAN    15\n",
       "Name: search_term, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_misclass.search_term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add compare function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to getting outliers using SVM model but diffferent criteria (more of a % than a absolute value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.score_samples(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10631984670769612\n"
     ]
    }
   ],
   "source": [
    "# Change treshhold as needed\n",
    "\n",
    "thresh = quantile(scores, 0.03)\n",
    "print(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# getting indexes\n",
    "\n",
    "index = where(scores<=thresh)\n",
    "index = list(index[0])\n",
    "print(len(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>guidislink</th>\n",
       "      <th>link</th>\n",
       "      <th>updated</th>\n",
       "      <th>updated_parsed</th>\n",
       "      <th>published</th>\n",
       "      <th>published_parsed</th>\n",
       "      <th>title</th>\n",
       "      <th>title_detail</th>\n",
       "      <th>summary</th>\n",
       "      <th>...</th>\n",
       "      <th>tags</th>\n",
       "      <th>arxiv_affiliation</th>\n",
       "      <th>arxiv_journal_ref</th>\n",
       "      <th>arxiv_doi</th>\n",
       "      <th>cleaning</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>category</th>\n",
       "      <th>tokens_merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://arxiv.org/abs/2203.06825v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2203.06825v1</td>\n",
       "      <td>2022-03-14T02:44:56Z</td>\n",
       "      <td>[2022, 3, 14, 2, 44, 56, 0, 73, 0]</td>\n",
       "      <td>2022-03-14T02:44:56Z</td>\n",
       "      <td>[2022, 3, 14, 2, 44, 56, 0, 73, 0]</td>\n",
       "      <td>Fairness Evaluation in Deepfake Detection Mode...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Fairness of deepfake detectors in the presence...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fairness of deepfake detectors in the presence...</td>\n",
       "      <td>[fairness, deepfake, detector, presence, anoma...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[2022, 3]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>fairness deepfake detector presence anomaly we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>http://arxiv.org/abs/2110.01640v1</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2110.01640v1</td>\n",
       "      <td>2021-10-04T18:02:56Z</td>\n",
       "      <td>[2021, 10, 4, 18, 2, 56, 0, 277, 0]</td>\n",
       "      <td>2021-10-04T18:02:56Z</td>\n",
       "      <td>[2021, 10, 4, 18, 2, 56, 0, 277, 0]</td>\n",
       "      <td>An Experimental Evaluation on Deepfake Detecti...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>Significant advances in deep learning have obt...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>significant advances in deep learning have obt...</td>\n",
       "      <td>[significant, advance, deep, learning, obtain,...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[2021, 10]</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>significant advance deep learning obtain hallm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>http://arxiv.org/abs/2103.09396v3</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/2103.09396v3</td>\n",
       "      <td>2021-10-03T01:05:56Z</td>\n",
       "      <td>[2021, 10, 3, 1, 5, 56, 6, 276, 0]</td>\n",
       "      <td>2021-03-17T01:48:34Z</td>\n",
       "      <td>[2021, 3, 17, 1, 48, 34, 2, 76, 0]</td>\n",
       "      <td>Pros and Cons of GAN Evaluation Measures: New ...</td>\n",
       "      <td>{'type': 'text/plain', 'language': None, 'base...</td>\n",
       "      <td>This work is an update of a previous paper on ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this work is an update of a previous paper on ...</td>\n",
       "      <td>[work, update, previous, paper, topic, publish...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[2021, 3]</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>work update previous paper topic publish year ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  guidislink  \\\n",
       "8    http://arxiv.org/abs/2203.06825v1        True   \n",
       "46   http://arxiv.org/abs/2110.01640v1        True   \n",
       "106  http://arxiv.org/abs/2103.09396v3        True   \n",
       "\n",
       "                                  link               updated  \\\n",
       "8    http://arxiv.org/abs/2203.06825v1  2022-03-14T02:44:56Z   \n",
       "46   http://arxiv.org/abs/2110.01640v1  2021-10-04T18:02:56Z   \n",
       "106  http://arxiv.org/abs/2103.09396v3  2021-10-03T01:05:56Z   \n",
       "\n",
       "                          updated_parsed             published  \\\n",
       "8     [2022, 3, 14, 2, 44, 56, 0, 73, 0]  2022-03-14T02:44:56Z   \n",
       "46   [2021, 10, 4, 18, 2, 56, 0, 277, 0]  2021-10-04T18:02:56Z   \n",
       "106   [2021, 10, 3, 1, 5, 56, 6, 276, 0]  2021-03-17T01:48:34Z   \n",
       "\n",
       "                        published_parsed  \\\n",
       "8     [2022, 3, 14, 2, 44, 56, 0, 73, 0]   \n",
       "46   [2021, 10, 4, 18, 2, 56, 0, 277, 0]   \n",
       "106   [2021, 3, 17, 1, 48, 34, 2, 76, 0]   \n",
       "\n",
       "                                                 title  \\\n",
       "8    Fairness Evaluation in Deepfake Detection Mode...   \n",
       "46   An Experimental Evaluation on Deepfake Detecti...   \n",
       "106  Pros and Cons of GAN Evaluation Measures: New ...   \n",
       "\n",
       "                                          title_detail  \\\n",
       "8    {'type': 'text/plain', 'language': None, 'base...   \n",
       "46   {'type': 'text/plain', 'language': None, 'base...   \n",
       "106  {'type': 'text/plain', 'language': None, 'base...   \n",
       "\n",
       "                                               summary  ...  \\\n",
       "8    Fairness of deepfake detectors in the presence...  ...   \n",
       "46   Significant advances in deep learning have obt...  ...   \n",
       "106  This work is an update of a previous paper on ...  ...   \n",
       "\n",
       "                                                  tags arxiv_affiliation  \\\n",
       "8    [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "46   [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...               NaN   \n",
       "106  [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...               NaN   \n",
       "\n",
       "    arxiv_journal_ref arxiv_doi  \\\n",
       "8                 NaN       NaN   \n",
       "46                NaN       NaN   \n",
       "106               NaN       NaN   \n",
       "\n",
       "                                              cleaning  \\\n",
       "8    fairness of deepfake detectors in the presence...   \n",
       "46   significant advances in deep learning have obt...   \n",
       "106  this work is an update of a previous paper on ...   \n",
       "\n",
       "                                                tokens  year  month_year  \\\n",
       "8    [fairness, deepfake, detector, presence, anoma...  2022   [2022, 3]   \n",
       "46   [significant, advance, deep, learning, obtain,...  2021  [2021, 10]   \n",
       "106  [work, update, previous, paper, topic, publish...  2021   [2021, 3]   \n",
       "\n",
       "    category                                      tokens_merged  \n",
       "8      cs.CV  fairness deepfake detector presence anomaly we...  \n",
       "46     cs.CV  significant advance deep learning obtain hallm...  \n",
       "106    cs.LG  work update previous paper topic publish year ...  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating second df\n",
    "df_misclass_2 = df[df.index.isin(index)]\n",
    "\n",
    "# And viewing it\n",
    "df_misclass_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'contamination': 0.01, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 100, 'n_jobs': None, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Sync/2022_SABRINA_CLIMATE/sab22_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "iso_model = IsolationForest(n_estimators=100,\n",
    "                  max_samples='auto',\n",
    "                  contamination=float(0.01),\n",
    "                  random_state=42\n",
    "                  )\n",
    "\n",
    "# Fitting model\n",
    "iso_model.fit(doc2vec_vectors)\n",
    "\n",
    "print(iso_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/models/arxiv_deepfake_iso_model.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(iso_model, MODEL_PATH + filename.split('.')[0] + \"_iso_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesselehrke/Sync/2022_SABRINA_CLIMATE/sab22_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/jesselehrke/Sync/2022_SABRINA_CLIMATE/sab22_venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "data['scores'] = iso_model.decision_function(doc2vec_vectors)\n",
    "\n",
    "data['anomaly_score'] = iso_model.predict(doc2vec_vectors) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>anomaly_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.108561</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>-0.064188</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>-0.080612</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>-0.089985</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-0.079779</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>-0.062221</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>-0.035550</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>-0.002202</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>-0.099722</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>-0.092948</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>-0.012949</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>-0.024181</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        scores  anomaly_score\n",
       "47   -0.108561             -1\n",
       "216  -0.064188             -1\n",
       "235  -0.080612             -1\n",
       "293  -0.089985             -1\n",
       "344  -0.079779             -1\n",
       "478  -0.000009             -1\n",
       "771  -0.062221             -1\n",
       "817  -0.035550             -1\n",
       "918  -0.002202             -1\n",
       "982  -0.099722             -1\n",
       "1027 -0.092948             -1\n",
       "1102 -0.012949             -1\n",
       "1137 -0.024181             -1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['anomaly_score']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: 13\n"
     ]
    }
   ],
   "source": [
    "outlier_index = where(data['anomaly_score'] == -1)\n",
    "indexes = list(outlier_index[0])\n",
    "\n",
    "print('Outliers: ' + str(len(indexes)))\n",
    "\n",
    "isolation_misclass = df[df.index.isin(indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAN    10\n",
       "Name: search_term, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolation_misclass.search_term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modelling of small-scale processes is a major source of error in climate\n",
      "models, hindering the accuracy of low-cost models which must approximate such\n",
      "processes through parameterization. Using stochasticity and machine learning\n",
      "have led to better models but there is a lack of work on combining the benefits\n",
      "from both. We show that by using a physically-informed recurrent neural network\n",
      "within a probabilistic framework, our resulting model for the Lorenz 96\n",
      "atmospheric simulation is competitive and often superior to both a bespoke\n",
      "baseline and an existing probabilistic machine-learning (GAN) one. This is due\n",
      "to a superior ability to model temporal correlations compared to standard\n",
      "first-order autoregressive schemes. The model also generalises to unseen\n",
      "regimes. We evaluate across a number of metrics from the literature, but also\n",
      "discuss how the probabilistic metric of likelihood may be a unifying choice for\n",
      "future probabilistic climate models.\n",
      "-------------------------------\n",
      "The increasingly crucial role of human displacements in complex societal\n",
      "phenomena, such as traffic congestion, segregation, and the diffusion of\n",
      "epidemics, is attracting the interest of scientists from several disciplines.\n",
      "In this article, we address mobility network generation, i.e., generating a\n",
      "city's entire mobility network, a weighted directed graph in which nodes are\n",
      "geographic locations and weighted edges represent people's movements between\n",
      "those locations, thus describing the entire mobility set flows within a city.\n",
      "Our solution is MoGAN, a model based on Generative Adversarial Networks (GANs)\n",
      "to generate realistic mobility networks. We conduct extensive experiments on\n",
      "public datasets of bike and taxi rides to show that MoGAN outperforms the\n",
      "classical Gravity and Radiation models regarding the realism of the generated\n",
      "networks. Our model can be used for data augmentation and performing\n",
      "simulations and what-if analysis.\n",
      "-------------------------------\n",
      "The literature has proposed several methods to finetune pretrained GANs on\n",
      "new datasets, which typically results in higher performance compared to\n",
      "training from scratch, especially in the limited-data regime. However, despite\n",
      "the apparent empirical benefits of GAN pretraining, its inner mechanisms were\n",
      "not analyzed in-depth, and understanding of its role is not entirely clear.\n",
      "Moreover, the essential practical details, e.g., selecting a proper pretrained\n",
      "GAN checkpoint, currently do not have rigorous grounding and are typically\n",
      "determined by trial and error.\n",
      "  This work aims to dissect the process of GAN finetuning. First, we show that\n",
      "initializing the GAN training process by a pretrained checkpoint primarily\n",
      "affects the model's coverage rather than the fidelity of individual samples.\n",
      "Second, we explicitly describe how pretrained generators and discriminators\n",
      "contribute to the finetuning process and explain the previous evidence on the\n",
      "importance of pretraining both of them. Finally, as an immediate practical\n",
      "benefit of our analysis, we describe a simple recipe to choose an appropriate\n",
      "GAN checkpoint that is the most suitable for finetuning to a particular target\n",
      "task. Importantly, for most of the target tasks, Imagenet-pretrained GAN,\n",
      "despite having poor visual quality, appears to be an excellent starting point\n",
      "for finetuning, resembling the typical pretraining scenario of discriminative\n",
      "computer vision models.\n",
      "-------------------------------\n",
      "MD-GAN is a machine learning-based method that can evolve part of the system\n",
      "at any time step, accelerating the generation of molecular dynamics data. For\n",
      "the accurate prediction of MD-GAN, sufficient information on the dynamics of a\n",
      "part of the system should be included with the training data. Therefore, the\n",
      "selection of the part of the system is important for efficient learning. In a\n",
      "previous study, only one particle (or vector) of each molecule was extracted as\n",
      "part of the system. Therefore, we investigated the effectiveness of adding\n",
      "information from other particles to the learning process. In the experiment of\n",
      "the polyethylene system, when the dynamics of three particles of each molecule\n",
      "were used, the diffusion was successfully predicted using one-third of the time\n",
      "length of the training data, compared to the single-particle input.\n",
      "Surprisingly, the unobserved transition of diffusion in the training data was\n",
      "also predicted using this method.\n",
      "-------------------------------\n",
      "In recent years, hyperspectral image (HSI) classification based on generative\n",
      "adversarial networks (GAN) has achieved great progress. GAN-based\n",
      "classification methods can mitigate the limited training sample dilemma to some\n",
      "extent. However, several studies have pointed out that existing GAN-based HSI\n",
      "classification methods are heavily affected by the imbalanced training data\n",
      "problem. The discriminator in GAN always contradicts itself and tries to\n",
      "associate fake labels to the minority-class samples, and thus impair the\n",
      "classification performance. Another critical issue is the mode collapse in\n",
      "GAN-based methods. The generator is only capable of producing samples within a\n",
      "narrow scope of the data space, which severely hinders the advancement of\n",
      "GAN-based HSI classification methods. In this paper, we proposed an Adaptive\n",
      "DropBlock-enhanced Generative Adversarial Networks (ADGAN) for HSI\n",
      "classification. First, to solve the imbalanced training data problem, we adjust\n",
      "the discriminator to be a single classifier, and it will not contradict itself.\n",
      "Second, an adaptive DropBlock (AdapDrop) is proposed as a regularization method\n",
      "employed in the generator and discriminator to alleviate the mode collapse\n",
      "issue. The AdapDrop generated drop masks with adaptive shapes instead of a\n",
      "fixed size region, and it alleviates the limitations of DropBlock in dealing\n",
      "with ground objects with various shapes. Experimental results on three HSI\n",
      "datasets demonstrated that the proposed ADGAN achieved superior performance\n",
      "over state-of-the-art GAN-based methods. Our codes are available at\n",
      "https://github.com/summitgao/HC_ADGAN\n",
      "-------------------------------\n",
      "Discovering meaningful directions in the latent space of GANs to manipulate\n",
      "semantic attributes typically requires large amounts of labeled data. Recent\n",
      "work aims to overcome this limitation by leveraging the power of Contrastive\n",
      "Language-Image Pre-training (CLIP), a joint text-image model. While promising,\n",
      "these methods require several hours of preprocessing or training to achieve the\n",
      "desired manipulations. In this paper, we present StyleMC, a fast and efficient\n",
      "method for text-driven image generation and manipulation. StyleMC uses a\n",
      "CLIP-based loss and an identity loss to manipulate images via a single text\n",
      "prompt without significantly affecting other attributes. Unlike prior work,\n",
      "StyleMC requires only a few seconds of training per text prompt to find stable\n",
      "global directions, does not require prompt engineering and can be used with any\n",
      "pre-trained StyleGAN2 model. We demonstrate the effectiveness of our method and\n",
      "compare it to state-of-the-art methods. Our code can be found at\n",
      "http://catlab-team.github.io/stylemc.\n",
      "-------------------------------\n",
      "GaN based high electron mobility transistors show promise in numerous device\n",
      "applications which elicits the need for accurate models of bulk, surface, and\n",
      "interface electronic properties. We detail here a hybrid density functional\n",
      "theory study of zinc blende (zb) GaN and diamond bulk and surface properties,\n",
      "and zb GaN on diamond interfaces using slab supercell models. Details are\n",
      "provided on the dependence of electronic properties with respect to supercell\n",
      "size, the use of pseudo-hydrogen to passivate the bottom GaN layer, and dipole\n",
      "corrections. The large bulk modulus of diamond provides a templating structure\n",
      "for GaN to grow upon, where a large lattice mismatch is accounted for through\n",
      "the inclusion of a cationic Ga adlayer. Looking at both type I and II surfaces\n",
      "and interfaces of GaN shows the instability of zb GaN without an adlayer (type\n",
      "II), where increased size, pseudo-hydrogen passivation and dipole corrections\n",
      "do not remove the spurious interaction between the top and bottom layers in\n",
      "type II GaN. Layer dependent density of states, local potential differences,\n",
      "and charge density differences show that the type I interface (with a Ga\n",
      "adlayer) is stable with an adhesion energy of 0.704 eV/{\\AA}2 (4.346 J/m2);\n",
      "interestingly, the diamond charge density intercalates into the first layer of\n",
      "GaN, which was seen experimentally for wurtzite GaN grown over diamond. The\n",
      "type II interface is shown to be unstable which implies that, to form a stable,\n",
      "thin-film zb interface between GaN and diamond, the partial pressure of\n",
      "trimethylgallium must be controlled to ensure a Ga layer exists both on the top\n",
      "and bottom layer of the GaN thin film atop the diamond. We believe our results\n",
      "can shed light towards a better understanding of the GaN/diamond multifaceted\n",
      "interface present in the GaN overgrowth on diamond samples.\n",
      "-------------------------------\n",
      "As a promising tool to navigate in the vast chemical space, artificial\n",
      "intelligence (AI) is leveraged for drug design. From the year 2017 to 2021, the\n",
      "number of applications of several recent AI models (i.e. graph neural network\n",
      "(GNN), recurrent neural network (RNN), variation autoencoder (VAE), generative\n",
      "adversarial network (GAN), flow and reinforcement learning (RL)) in drug design\n",
      "increases significantly. Many relevant literature reviews exist. However, none\n",
      "of them provides an in-depth summary of many applications of the recent AI\n",
      "models in drug design. To complement the existing literature, this survey\n",
      "includes the theoretical development of the previously mentioned AI models and\n",
      "detailed summaries of 42 recent applications of AI in drug design. Concretely,\n",
      "13 of them leverage GNN for molecular property prediction and 29 of them use RL\n",
      "and/or deep generative models for molecule generation and optimization. In most\n",
      "cases, the focus of the summary is the models, their variants, and\n",
      "modifications for specific tasks in drug design. Moreover, 60 additional\n",
      "applications of AI in molecule generation and optimization are briefly\n",
      "summarized in a table. Finally, this survey provides a holistic discussion of\n",
      "the abundant applications so that the tasks, potential solutions, and\n",
      "challenges in AI-based drug design become evident.\n",
      "-------------------------------\n",
      "A myriad of recent literary works has leveraged generative adversarial\n",
      "networks (GANs) to spawn unseen evasion samples. The purpose is to annex the\n",
      "generated data with the original train set for adversarial training to improve\n",
      "the detection performance of machine learning (ML) classifiers. The quality of\n",
      "generating adversarial samples relies on the adequacy of training data samples.\n",
      "However, in low data regimes like medical diagnostic imaging and cybersecurity,\n",
      "the anomaly samples are scarce in number. This paper proposes a novel GAN\n",
      "design called Evasion Generative Adversarial Network (EVAGAN) that is more\n",
      "suitable for low data regime problems that use oversampling for detection\n",
      "improvement of ML classifiers. EVAGAN not only can generate evasion samples,\n",
      "but its discriminator can act as an evasion aware classifier. We have\n",
      "considered Auxiliary Classifier GAN (ACGAN) as a benchmark to evaluate the\n",
      "performance of EVAGAN on cybersecurity (ISCX-2014, CIC-2017 and CIC2018) botnet\n",
      "and computer vision (MNIST) datasets. We demonstrate that EVAGAN outperforms\n",
      "ACGAN for unbalanced datasets with respect to detection performance, training\n",
      "stability and time complexity. EVAGAN's generator quickly learns to generate\n",
      "the low sample class and hardens its discriminator simultaneously. In contrast\n",
      "to ML classifiers that require security hardening after being adversarially\n",
      "trained by GAN generated data, EVAGAN renders it needless. The experimental\n",
      "analysis proves that EVAGAN is an efficient evasion hardened model for low data\n",
      "regimes for the selected cybersecurity and computer vision datasets. Code will\n",
      "be available at https://github.com/rhr407/EVAGAN.\n",
      "-------------------------------\n",
      "Training real-world neural network models to achieve high performance and\n",
      "generalizability typically requires a substantial amount of labeled data,\n",
      "spanning a broad range of variation. This data-labeling process can be both\n",
      "labor and cost intensive. To achieve desirable predictive performance, a\n",
      "trained model is typically applied into a domain where the data distribution is\n",
      "similar to the training dataset. However, for many agricultural machine\n",
      "learning problems, training datasets are collected at a specific location,\n",
      "during a specific period in time of the growing season. Since agricultural\n",
      "systems exhibit substantial variability in terms of crop type, cultivar,\n",
      "management, seasonal growth dynamics, lighting condition, sensor type, etc, a\n",
      "model trained from one dataset often does not generalize well across domains.\n",
      "To enable more data efficient and generalizable neural network models in\n",
      "agriculture, we propose a method that generates photorealistic agricultural\n",
      "images from a synthetic 3D crop model domain into real world crop domains. The\n",
      "method uses a semantically constrained GAN (generative adversarial network) to\n",
      "preserve the fruit position and geometry. We observe that a baseline CycleGAN\n",
      "method generates visually realistic target domain images but does not preserve\n",
      "fruit position information while our method maintains fruit positions well.\n",
      "Image generation results in vineyard grape day and night images show the visual\n",
      "outputs of our network are much better compared to a baseline network.\n",
      "Incremental training experiments in vineyard grape detection tasks show that\n",
      "the images generated from our method can significantly speed the domain\n",
      "adaption process, increase performance for a given number of labeled images\n",
      "(i.e. data efficiency), and decrease labeling requirements.\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "for c in isolation_misclass.summary:\n",
    "      print(c)\n",
    "      print('-------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Simularity Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, fit and predict\n",
    "auto_encoder = MLPRegressor(hidden_layer_sizes=(\n",
    "                                                 600,\n",
    "                                                 150, \n",
    "                                                 600,\n",
    "                                               ))\n",
    "\n",
    "auto_encoder.fit(doc2vec_vectors, doc2vec_vectors)\n",
    "\n",
    "predicted_vectors = auto_encoder.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual loss\n",
    "pd.DataFrame(auto_encoder.loss_curve_).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_consine_similarity(tupple):\n",
    "    return tupple[1]\n",
    "\n",
    "def get_computed_similarities(vectors, predicted_vectors, reverse=False):\n",
    "    data_size = len(df)\n",
    "    cosine_similarities = []\n",
    "    for i in range(data_size):\n",
    "        cosine_sim_val = (1 - cosine(vectors[i], predicted_vectors[i]))\n",
    "        cosine_similarities.append((i, cosine_sim_val))\n",
    "\n",
    "    return sorted(cosine_similarities, key=key_consine_similarity, reverse=reverse)\n",
    "\n",
    "def display_top_n(sorted_cosine_similarities, n=5):\n",
    "    for i in range(n):\n",
    "        index, consine_sim_val = sorted_cosine_similarities[i]\n",
    "        print('Title: ', df.iloc[index, 7])\n",
    "        print('ID: ', df.iloc[index, 0])  \n",
    "        print('Cosine Sim Val :', consine_sim_val)\n",
    "        print('---------------------------------')\n",
    "\n",
    "# add function to sort by percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify how many 'outliers' you want to see\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top n unique')\n",
    "\n",
    "sorted_cosine_similarities = get_computed_similarities(vectors=doc2vec_vectors, predicted_vectors=predicted_vectors)\n",
    "\n",
    "display_top_n(sorted_cosine_similarities=sorted_cosine_similarities, n = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the cosines - will revise during first test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn histogram\n",
    "# Can use to adjust the N above (or percent, once we have that function) to see the low cluster\n",
    "\n",
    "sns.distplot(losses, hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "\n",
    "# # Add labels\n",
    "# plt.title('Title')\n",
    "# plt.xlabel('Label x')\n",
    "# plt.ylabel('Label y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIRC not fully functional yet - for more Cosine work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_unique_index, cosine_sim_val = sorted_cosine_similarities[0]\n",
    "print(most_unique_index)\n",
    "most_unique_plot =df.iloc[most_unique_index, 9] # index here matters!\n",
    "most_unique_words_counter = Counter(preprocess_string(most_unique_plot))\n",
    "print(most_unique_words_counter)\n",
    "\n",
    "# intersected_common_word_counter = common_word_counter & most_unique_words_counter\n",
    "\n",
    "# intersected_common_words = [word[0] for word in intersected_common_word_counter.items()]\n",
    "# intersected_common_word_counts = [word[1] for word in intersected_common_word_counter.items()]\n",
    "\n",
    "# intersected_common_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73afed2e6e5cae272ca1e451939a06651cb7194d426a79bc157d2d09ec23572e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('sab22_venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
