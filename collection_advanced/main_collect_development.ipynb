{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import lxml\n",
    "\n",
    "import json\n",
    "#from dateutil import rrule\n",
    "\n",
    "## needs a py file to import properly in this structure\n",
    "#from utils.collection_utils import datetime_parse\n",
    "\n",
    "from itertools import combinations, permutations, chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "IN_DATA_PATH = '../data/input_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Datetime function needs work\n",
    "\n",
    "If the month dict makes a replacement, need to tag that as a month and limit my permutations as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_parse(x):\n",
    "      '''\n",
    "      Parse datetime out of a string\n",
    "      More functionality should be added as issues encountered \n",
    "      '''\n",
    "      # Remove all non-alpha-numeric\n",
    "      out = re.sub(r'[^0-9a-zA-Z:]+', ' ', x)\n",
    "\n",
    "      ## Remove any starting tag with :\n",
    "      # str.split('Updated: ', expand=True)\n",
    "      \n",
    "      # Remove time \n",
    "      # .str.replace(r'\\b(([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9](:[0-5][0-9])?\\s?([AaPp][Mm])?)', ' ')\n",
    "      \n",
    "      month_dict = {\n",
    "      'January':'1', 'February':'2', 'March':'3', 'April':'4', 'May':'5', 'June':'6',\n",
    "      'July':'7', 'August':'8', 'September':'9', 'October':'10', 'November':'11', \n",
    "      'December':'12', 'Jan':'1', 'Feb':'2', 'Mar':'3', 'Apr':'4', 'May':'5',\n",
    "      'Jun':'6', 'Jul':'7', 'Aug':'8', 'Sep':'9', 'Oct':'10', 'Nov':'11', 'Dec':'12'\n",
    "      }\n",
    "\n",
    "      # Removing digits longer than 4 long and words not in months\n",
    "      #out = [out.replace(key, value) for key, value in month_dict.items() if key in out][0]\n",
    "      \n",
    "      # Removing digits longer than 4 long and words not in months (now redundant)\n",
    "      #out = re.sub(r'[0-9]\\d{4,}', ' ', out)\n",
    "      #out = re.sub(r'[0-9]+:[0-9]+', ' ', out) # all after : not tested\n",
    "\n",
    "      out = out.split(' ')\n",
    "      out = [word for word in out if word.isdigit() or word in list(month_dict.keys())]\n",
    "      \n",
    "      out = ' '.join(out[:3])\n",
    "      # Strip loose whitespace\n",
    "\n",
    "      out = out.strip()\n",
    "\n",
    "      # Lists for parsing\n",
    "      month = ['%b', '%m', '%B']\n",
    "      day = ['%d']\n",
    "      year = ['%Y', '%y']\n",
    "\n",
    "      varieties = list(permutations(chain(month,day,year), 3))\n",
    "      for v in varieties:\n",
    "            v = ' '.join(v)\n",
    "            try:\n",
    "                  date = [datetime.strptime(str(out), v)]# if d != 0 else d for d in out] #old tag used elsewhere, here for ref.\n",
    "\n",
    "                  if date is not None:\n",
    "                        print('Successfully parsed with format: ' + v)\n",
    "                        return date\n",
    "                        break\n",
    "            except:\n",
    "                  #print('Failed: ' + v)\n",
    "                  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To load as dataframe: no value at present \n",
    "\n",
    "# load_file = IN_DATA_PATH + 'collection_urls_df.jsonl'\n",
    "\n",
    "# df = pd.read_json(load_file, convert_dates=True, lines=True, orient='records')\n",
    "# df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file = IN_DATA_PATH + 'collection_urls_dict.json'\n",
    "\n",
    "with open(load_file) as handle:\n",
    "    sources = json.loads(handle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNA_Russia': 'https://www.cna.org/centers/cna/sppp/rsp/russia-ai-archive#newsletters',\n",
       " 'CNA_China': 'https://www.cna.org/centers/cna/cip/china/china-ai-newsletter',\n",
       " 'MIT_Technology_Review': 'https://www.technologyreview.com/',\n",
       " 'Synced': 'https://syncedreview.com/',\n",
       " 'IEEE_spectrum': 'https://spectrum.ieee.org/',\n",
       " 'Import_AI': 'https://us13.campaign-archive.com/home/?u=67bd06787e84d73db24fb0aa5&id=6c9d98ff2c',\n",
       " 'ChinAI': 'https://chinai.substack.com/archive?sort=new'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 9, 46, 7, 425081)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get today's date\n",
    "# To do: create a \"last scraped json\"\n",
    "today = datetime.now()\n",
    "\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for saving collected data\n",
    "\n",
    "title_list = []\n",
    "url_list = []\n",
    "dates = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = sources['Import_AI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/11/2022 - Import AI 291: Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company\n",
      "http://eepurl.com/hZjozT\n",
      "04/05/2022 - Import AI 290: China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data\n",
      "http://eepurl.com/hYSyi9\n",
      "03/28/2022 - Import AI 289: Copyright v AI art; NIST tries to measure bias in AI; solar-powered Markov chains\n",
      "http://eepurl.com/hYaLin\n",
      "03/21/2022 - Import AI 288: Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless.\n",
      "http://eepurl.com/hXCPbv\n",
      "03/07/2022 - Import AI 287: 10 exaflop supercomputer; Google deploys differential privacy; humans can outsmart deepfakes pretty well\n",
      "http://eepurl.com/hWsYwH\n",
      "02/28/2022 - Import AI 286: Fairness through dumbness; planet-scale AI computing; another AI safety startup appears\n",
      "http://eepurl.com/hVSMf1\n",
      "02/21/2022 - Import AI 285: RL+Fusion; why RL demands better public policy; Cohere raises $125m\n",
      "http://eepurl.com/hVhHez\n",
      "02/14/2022 - Import AI 284: 20bn GPT model; diachronic LMs; what people think about AI\n",
      "http://eepurl.com/hUEhnX\n",
      "02/07/2022 - Import AI 283: Open source 20B GPT3; Chinese researchers make better adversarial example attacks; Mozilla launches AI auditing project.\n",
      "http://eepurl.com/hT5gnr\n",
      "02/01/2022 - Import AI 282: Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition\n",
      "http://eepurl.com/hTyKXj\n",
      "01/24/2022 - Import AI 281: China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs\n",
      "http://eepurl.com/hSYp5z\n",
      "01/17/2022 - Import AI 280: Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet\n",
      "http://eepurl.com/hSmoov\n",
      "01/10/2022 - Import AI 279: Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance\n",
      "http://eepurl.com/hRQPeD\n",
      "12/27/2021 - Import AI 278: Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI\n",
      "http://eepurl.com/hQZI61\n",
      "12/13/2021 - Import AI 277: DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs\n",
      "http://eepurl.com/hPQFJv\n",
      "12/06/2021 - Import AI 276: Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like\n",
      "http://eepurl.com/hPe-3n\n",
      "11/22/2021 - Import AI 275: Facebook dreams of a world-spanning neural net; Microsoft announces a 30-petaflop supercomputer; FTC taps AI Now for AI advice\n",
      "http://eepurl.com/hN0Cdb\n",
      "11/15/2021 - Import AI 274: Multilingual models cement power structures; a giant British Sign Language dataset;  and benchmarks for the UN SDGs\n",
      "http://eepurl.com/hNnGtb\n",
      "11/08/2021 - Import AI 273: Corruption VS Surveillance; Baidu makes better object detection; understanding the legal risk of datasets\n",
      "http://eepurl.com/hMRZZ9\n",
      "11/01/2021 - Import AI #272: AGI-never or AGI-soon?, simulating stock markets; evaluating unsupervised RL\n",
      "http://eepurl.com/hMdXB1\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Get list of all Import AI issues with date, title, and link\n",
    "\n",
    "response = requests.post(base_url) #headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')\n",
    "\n",
    "objects = html.find_all('li', class_=\"campaign\")\n",
    "for obj in objects:\n",
    "      print(obj.text)\n",
    "      print(obj.a['href'])\n",
    "      title_list.append(obj.text)\n",
    "      url_list.append(obj.a['href'])\n",
    "\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04 11 2022 Import AI 291: Google trains the world s biggest language model how robots can be smarter about the world Conjecture a new AI alignment company\n",
      "Successfully parsed with format: %m %d %Y\n",
      "04 05 2022 Import AI 290: China plans massive models DeepMind makes a smaller and smarter model open source CLIP data\n",
      "Successfully parsed with format: %m %d %Y\n",
      "03 28 2022 Import AI 289: Copyright v AI art NIST tries to measure bias in AI solar powered Markov chains\n",
      "Successfully parsed with format: %m %d %Y\n",
      "03 21 2022 Import AI 288: Chinese researchers try to train 100trillion brain scale models 33 of AI benchmarks are meaningless \n",
      "Successfully parsed with format: %m %d %Y\n",
      "03 07 2022 Import AI 287: 10 exaflop supercomputer Google deploys differential privacy humans can outsmart deepfakes pretty well\n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 28 2022 Import AI 286: Fairness through dumbness planet scale AI computing another AI safety startup appears\n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 21 2022 Import AI 285: RL Fusion why RL demands better public policy Cohere raises 125m\n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 14 2022 Import AI 284: 20bn GPT model diachronic LMs what people think about AI\n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 07 2022 Import AI 283: Open source 20B GPT3 Chinese researchers make better adversarial example attacks Mozilla launches AI auditing project \n",
      "Successfully parsed with format: %m %d %Y\n",
      "02 01 2022 Import AI 282: Facebook s AI supercomputer Anduril gets a SOCOM contract Twitter talks about running an algo bias competition\n",
      "Successfully parsed with format: %m %d %Y\n",
      "01 24 2022 Import AI 281: China does more surveillance research than US and Europe Google reveals its text model LaMDA Microsoft improves MoEs\n",
      "Successfully parsed with format: %m %d %Y\n",
      "01 17 2022 Import AI 280: Why bigger is worse for RL AI generated Pokemon real world EfficientNet\n",
      "Successfully parsed with format: %m %d %Y\n",
      "01 10 2022 Import AI 279: Baidu adds knowledge to a language model US military AI how China thinks about AI governance\n",
      "Successfully parsed with format: %m %d %Y\n",
      "12 27 2021 Import AI 278: Can we ever trust an AI what the future of semiconductors looks like better images of AI\n",
      "Successfully parsed with format: %m %d %Y\n",
      "12 13 2021 Import AI 277: DeepMind builds a GPT 3 model Catalan GLUE FTC plans AI regs\n",
      "Successfully parsed with format: %m %d %Y\n",
      "12 06 2021 Import AI 276: Tracking journalists with computer vision spotting factory defects with AI and what simulated war might look like\n",
      "Successfully parsed with format: %m %d %Y\n",
      "11 22 2021 Import AI 275: Facebook dreams of a world spanning neural net Microsoft announces a 30 petaflop supercomputer FTC taps AI Now for AI advice\n",
      "Successfully parsed with format: %m %d %Y\n",
      "11 15 2021 Import AI 274: Multilingual models cement power structures a giant British Sign Language dataset and benchmarks for the UN SDGs\n",
      "Successfully parsed with format: %m %d %Y\n",
      "11 08 2021 Import AI 273: Corruption VS Surveillance Baidu makes better object detection understanding the legal risk of datasets\n",
      "Successfully parsed with format: %m %d %Y\n",
      "11 01 2021 Import AI 272: AGI never or AGI soon simulating stock markets evaluating unsupervised RL\n",
      "Successfully parsed with format: %m %d %Y\n"
     ]
    }
   ],
   "source": [
    "# Parse date from list\n",
    "\n",
    "for item in title_list:\n",
    "      date_sequence = datetime_parse(item)\n",
    "      dates.append(date_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://eepurl.com/hZjozT</td>\n",
       "      <td>04/11/2022 - Import AI 291: Google trains the ...</td>\n",
       "      <td>2022-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://eepurl.com/hYSyi9</td>\n",
       "      <td>04/05/2022 - Import AI 290: China plans massiv...</td>\n",
       "      <td>2022-04-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        url  \\\n",
       "0  http://eepurl.com/hZjozT   \n",
       "1  http://eepurl.com/hYSyi9   \n",
       "\n",
       "                                               title       date  \n",
       "0  04/11/2022 - Import AI 291: Google trains the ... 2022-04-11  \n",
       "1  04/05/2022 - Import AI 290: China plans massiv... 2022-04-05  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save collection to dataframe\n",
    "\n",
    "df_collected = pd.DataFrame(list(zip(title_list, url_list, dates)), \n",
    "            columns=['title', 'url', 'date'])\n",
    "\n",
    "df_collected.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'title', 'date'], dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter keywords in json referenced here\n",
    "\n",
    "# For testing only\n",
    "#search_terms = ['DeepMind', 'Google']\n",
    "\n",
    "load_file = IN_DATA_PATH + 'collection_searchterms.json'\n",
    "\n",
    "with open(load_file) as handle:\n",
    "    search_terms = json.loads(handle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeepMind', 'Google']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms['search_term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to save to\n",
    "relevant_text = []\n",
    "\n",
    "#preping list fo dictionary conversion\n",
    "relevant_text.append(['title', 'url', 'date', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: http://eepurl.com/hZjozT\n",
      "True\n",
      "Fetching: http://eepurl.com/hYSyi9\n",
      "True\n",
      "Fetching: http://eepurl.com/hYaLin\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Gets text for results within timedelta window\n",
    "#     This is probably best changed to a \"last scraped\" date from a json\n",
    "\n",
    "for index, row in df_collected.iterrows():\n",
    "      if row.date >= today - timedelta(days=30):\n",
    "            print('Fetching: ' + row.url)\n",
    "            response = requests.post(row.url) #headers=header)\n",
    "\n",
    "            html = soup(response.text, 'lxml')\n",
    "\n",
    "            objects = html.find_all('p')#, class_=\"campaign\")\n",
    "            issue_text = []\n",
    "            for obj in objects:\n",
    "                  issue_text.append(obj.text)\n",
    "                  #print(obj.text)\n",
    "            issue_text = ' '.join(issue_text)\n",
    "            #print(issue_text)\n",
    "            #print('--------------------------------')\n",
    "            \n",
    "            for word in search_terms['search_term']:\n",
    "                  if word in issue_text:\n",
    "                        save = list(row)\n",
    "                        save.append(issue_text)\n",
    "                        relevant_text.append(save)                  \n",
    "                        print('Search term found: ' +  word)\n",
    "                        break\n",
    "                  else: \n",
    "                        pass\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To save to json - not recommend as datetimes not serializable, would have to convert dt back to string\n",
    "# Note - ugly dictionary, e.g. not jsonl or indented, but kept simple unless requested\n",
    "\n",
    "# data_dict = {k: v for k, *v in zip(*relevant_text)}\n",
    "\n",
    "# with open('data.json', 'w') as f:\n",
    "#     json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(relevant_text[1:],columns=relevant_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: add if file clause\n",
    "old_df = pd.read_csv(DATA_PATH + 'open_ai_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([new_df, old_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If timing for collection and timedelta etc are good, should not be needed, but good to be sure\n",
    "# If you instead have date issues, you should do this in the collect loop to eliminate unneeeded collectio\n",
    "# e.g. if url in list(old_df.url): pass\n",
    "\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://eepurl.com/hZjozT</td>\n",
       "      <td>04/11/2022 - Import AI 291: Google trains the ...</td>\n",
       "      <td>2022-04-11 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://eepurl.com/hYSyi9</td>\n",
       "      <td>04/05/2022 - Import AI 290: China plans massiv...</td>\n",
       "      <td>2022-04-05 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://eepurl.com/hYaLin</td>\n",
       "      <td>03/28/2022 - Import AI 289: Copyright v AI art...</td>\n",
       "      <td>2022-03-28 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        url  \\\n",
       "0  http://eepurl.com/hZjozT   \n",
       "1  http://eepurl.com/hYSyi9   \n",
       "2  http://eepurl.com/hYaLin   \n",
       "\n",
       "                                               title                 date  \\\n",
       "0  04/11/2022 - Import AI 291: Google trains the ...  2022-04-11 00:00:00   \n",
       "1  04/05/2022 - Import AI 290: China plans massiv...  2022-04-05 00:00:00   \n",
       "2  03/28/2022 - Import AI 289: Copyright v AI art...  2022-03-28 00:00:00   \n",
       "\n",
       "                                                text  \n",
       "0  Welcome to Import AI, a newsletter about artif...  \n",
       "1  Welcome to Import AI, a newsletter about artif...  \n",
       "2  Welcome to Import AI, a newsletter about artif...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(DATA_PATH + 'open_ai_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://syncedreview.com/'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = sources['Synced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 9, 46, 7, 425081)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date part for url\n",
    "year = datetime.now().year\n",
    "month = datetime.now().month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://syncedreview.com/2022/4/'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_url = base_url + str(year) + '/' +  str(month) + '/'\n",
    "current_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists for saving\n",
    "title_list = []\n",
    "url_list = []\n",
    "summary_list = []\n",
    "date_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "# Setting up a fake useragent\n",
    "# Can do for all collections, but for now limited to where is seems necessary only\n",
    "# Ignore the error \n",
    "\n",
    "ua = UserAgent(verify_ssl=False, cache=False)\n",
    "\n",
    "user_agent = ua.random\n",
    "header = {'User-Agent': user_agent}\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')\n",
    "\n",
    "obj = html.find('div',id= \"primary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dates = obj.find_all(class_='entry-date')\n",
    "for d in dates:\n",
    "      d_parsed = datetime_parse(d.text)\n",
    "      date_list.append(d_parsed[0])\n",
    "      #date_list.append(d.text)\n",
    "\n",
    "titles = obj.find_all(class_='entry-title')\n",
    "for t in titles:\n",
    "      title_list.append(t.text)\n",
    "      url_list.append(t.a['href'])\n",
    "\n",
    "summaries = obj.find_all(class_='entry-summary')\n",
    "for s in summaries:\n",
    "      summary_list.append(s.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Tsinghua &amp; NKU’s Visual Attention Network Comb...</td>\n",
       "      <td>https://syncedreview.com/2022/02/23/deepmind-p...</td>\n",
       "      <td>2022-02-23</td>\n",
       "      <td>\\nIn the new paper Visual Attention Network, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>DeepMind Trains Agents to Control Computers as...</td>\n",
       "      <td>https://syncedreview.com/2022/02/22/deepmind-p...</td>\n",
       "      <td>2022-02-22</td>\n",
       "      <td>\\nDeepMind trains agents to use keyboard and m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "38  Tsinghua & NKU’s Visual Attention Network Comb...   \n",
       "39  DeepMind Trains Agents to Control Computers as...   \n",
       "\n",
       "                                                  url       date  \\\n",
       "38  https://syncedreview.com/2022/02/23/deepmind-p... 2022-02-23   \n",
       "39  https://syncedreview.com/2022/02/22/deepmind-p... 2022-02-22   \n",
       "\n",
       "                                              summary  \n",
       "38  \\nIn the new paper Visual Attention Network, a...  \n",
       "39  \\nDeepMind trains agents to use keyboard and m...  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collected2 = pd.DataFrame(list(zip(title_list, url_list, date_list, summary_list)), \n",
    "            columns=['title', 'url', 'date', 'summary'])\n",
    "\n",
    "df_collected2.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword = 'AlexNet'\n",
    "search_terms['search_term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_text = []\n",
    "\n",
    "relevant_text.append(['title', 'url', 'date', 'summary', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://syncedreview.com/2022/04/20/uc-berkeley-intels-photorealistic-denoising-method-boosts-video-quality-on-moonless-nights/\n",
      "Fetching: https://syncedreview.com/2022/04/19/toward-self-improving-neural-networks-schmidhuber-teams-scalable-self-referential-weight-matrix-learns-to-modify-itself/\n",
      "Fetching: https://syncedreview.com/2022/04/18/meet-deepdpm-no-predefined-number-of-clusters-needed-for-deep-clustering-tasks/\n",
      "Fetching: https://syncedreview.com/2022/04/14/alibabas-usi-a-unified-scheme-for-training-any-backbone-on-imagenet-that-delivers-top-results-without-hyperparameter-tuning/\n",
      "Fetching: https://syncedreview.com/2022/04/13/openais-unclip-text-to-image-system-leverages-contrastive-and-diffusion-models-to-achieve-sota-performance/\n",
      "Fetching: https://syncedreview.com/2022/04/12/google-builds-language-models-with-socratic-dialogue-to-improve-zero-shot-multimodal-reasoning-capabilities/\n",
      "Search term found: Google\n",
      "Fetching: https://syncedreview.com/2022/04/11/maryland-u-google-introduce-lilnetx-simultaneously-optimizing-dnn-size-cost-structured-sparsity-accuracy/\n",
      "Search term found: Google\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_collected2.iterrows():\n",
    "      if row.date >= today - timedelta(days=14):\n",
    "            print('Fetching: ' + row.url)\n",
    "            response = requests.post(row.url) #headers=header)\n",
    "\n",
    "            html = soup(response.text, 'lxml')\n",
    "\n",
    "            objects = html.find_all('div', class_=\"entry-content\")\n",
    "            article_text = []\n",
    "            for obj in objects:\n",
    "                  paragraph_text = obj.find_all('p')\n",
    "                  for p in paragraph_text:\n",
    "                        article_text.append(p.text)\n",
    "                  #print(obj.text)\n",
    "            article_text = ' '.join(article_text)\n",
    "\n",
    "            for word in search_terms['search_term']:\n",
    "                  if word in article_text:\n",
    "                        save = list(row)\n",
    "                        save.append(article_text)\n",
    "                        relevant_text.append(save)                  \n",
    "                        print('Search term found: ' +  word)\n",
    "                        break\n",
    "            else: \n",
    "                  pass\n",
    "\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(relevant_text[1:],columns=relevant_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Builds Language Models with Socratic Di...</td>\n",
       "      <td>https://syncedreview.com/2022/04/12/google-bui...</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>\\n In the new paper Socratic Models: Composing...</td>\n",
       "      <td>Large-scale language-based foundation models s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maryland U &amp; Google Introduce LilNetX: Simulta...</td>\n",
       "      <td>https://syncedreview.com/2022/04/11/maryland-u...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>\\nA team from the University of Maryland and G...</td>\n",
       "      <td>The current conventional wisdom on deep neural...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Google Builds Language Models with Socratic Di...   \n",
       "1  Maryland U & Google Introduce LilNetX: Simulta...   \n",
       "\n",
       "                                                 url       date  \\\n",
       "0  https://syncedreview.com/2022/04/12/google-bui... 2022-04-12   \n",
       "1  https://syncedreview.com/2022/04/11/maryland-u... 2022-04-11   \n",
       "\n",
       "                                             summary  \\\n",
       "0  \\n In the new paper Socratic Models: Composing...   \n",
       "1  \\nA team from the University of Maryland and G...   \n",
       "\n",
       "                                                text  \n",
       "0  Large-scale language-based foundation models s...  \n",
       "1  The current conventional wisdom on deep neural...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: add if file clause\n",
    "old_df = pd.read_csv(DATA_PATH + 'synced_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([new_df, old_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If timing for collection and timedelta etc are good, should not be needed, but good to be sure\n",
    "# If you instead have date issues, you should do this in the collect loop to eliminate unneeeded collectio\n",
    "# e.g. if url in list(old_df.url): pass\n",
    "\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://eepurl.com/hZjozT</td>\n",
       "      <td>04/11/2022 - Import AI 291: Google trains the ...</td>\n",
       "      <td>2022-04-11 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://eepurl.com/hYSyi9</td>\n",
       "      <td>04/05/2022 - Import AI 290: China plans massiv...</td>\n",
       "      <td>2022-04-05 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://eepurl.com/hYaLin</td>\n",
       "      <td>03/28/2022 - Import AI 289: Copyright v AI art...</td>\n",
       "      <td>2022-03-28 00:00:00</td>\n",
       "      <td>Welcome to Import AI, a newsletter about artif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        url  \\\n",
       "0  http://eepurl.com/hZjozT   \n",
       "1  http://eepurl.com/hYSyi9   \n",
       "2  http://eepurl.com/hYaLin   \n",
       "\n",
       "                                               title                 date  \\\n",
       "0  04/11/2022 - Import AI 291: Google trains the ...  2022-04-11 00:00:00   \n",
       "1  04/05/2022 - Import AI 290: China plans massiv...  2022-04-05 00:00:00   \n",
       "2  03/28/2022 - Import AI 289: Copyright v AI art...  2022-03-28 00:00:00   \n",
       "\n",
       "                                                text  \n",
       "0  Welcome to Import AI, a newsletter about artif...  \n",
       "1  Welcome to Import AI, a newsletter about artif...  \n",
       "2  Welcome to Import AI, a newsletter about artif...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(DATA_PATH + 'synced_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = sources['MIT_Technology_Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_designator = 'topic/'\n",
    "topic_tags = ['artificial-intelligence', 'humans-and-technology', 'computing'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.technologyreview.com/topic/artificial-intelligence'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_url = base_url + topic_designator + topic_tags[0]\n",
    "final_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in topic_tags:\n",
    "      final_url = base_url + topic_designator + tag\n",
    "      # then search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Server': 'nginx', 'Date': 'Fri, 22 Apr 2022 10:42:32 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Content-Length': '34115', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'ETag': 'W/\"2ed7b-nH1JQs3vy3bcAMjKg3IzOTe7W+Y\"', 'Cache-Control': 'max-age=300, must-revalidate', 'Content-Encoding': 'gzip', 'X-rq': 'hhn2 0 2 9980', 'Age': '0', 'X-Cache': 'miss', 'Vary': 'X-Mobile-Class, Accept-Encoding', 'Accept-Ranges': 'bytes', 'Strict-Transport-Security': 'max-age=31536000;includeSubdomains;preload'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(final_url)\n",
    "\n",
    "# print headers of response\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ROOT_URL = \"https://wp.technologyreview.com/wp-json/irving/v1/data/term_archive?category_name=artificial-intelligence&page=1\"\n",
    "'''\n",
    "API_ROOT_URL\n",
    "slug\n",
    "?\n",
    "requestType\n",
    "=\n",
    "query\n",
    "&page=1'''\n",
    "\n",
    "def searchApi():\n",
    "    endpoint = API_ROOT_URL\n",
    "    data = {\n",
    "        \"slug\": \"term_archive\",\n",
    "        \"requestType\": \"category_name\",\n",
    "        \"query\": \"artificial-intelligence\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(endpoint)#, data=data)\n",
    "        if(response.status_code == 200):\n",
    "            #print(response.json())\n",
    "            for msg in response:\n",
    "                print(msg)\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchApi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_Link = \"https://wp.technologyreview.com/wp-json/irving/v1/data/term_archive?category_name=\"\n",
    "#artificial-intelligence\"\n",
    "#&page=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Server': 'nginx', 'Date': 'Fri, 22 Apr 2022 11:19:32 GMT', 'Content-Type': 'application/json; charset=UTF-8', 'Content-Length': '22', 'Connection': 'keep-alive', 'X-Robots-Tag': 'noindex', 'Link': '<https://wp.technologyreview.com/wp-json/>; rel=\"https://api.w.org/\"', 'X-Content-Type-Options': 'nosniff', 'Access-Control-Expose-Headers': 'X-WP-Total, X-WP-TotalPages, Link', 'Access-Control-Allow-Headers': 'Authorization, X-WP-Nonce, Content-Disposition, Content-MD5, Content-Type', 'Access-Control-Allow-Origin': 'https://www.technologyreview.com', 'Cache-Control': 'max-age=60', 'Allow': 'GET', 'X-rq': 'hhn2 0 4 9980', 'Content-Encoding': 'gzip', 'Age': '0', 'X-Cache': 'miss', 'Vary': 'Accept-Encoding, Origin', 'Accept-Ranges': 'bytes', 'Strict-Transport-Security': 'max-age=31536000;includeSubdomains;preload'}\n"
     ]
    }
   ],
   "source": [
    "# Making a get test request\n",
    "try:\n",
    "      response = requests.get(API_Link)\n",
    "\n",
    "      # print headers of response\n",
    "      if(response.status_code == 200):\n",
    "            print(response.headers)\n",
    "            \n",
    "except Exception:\n",
    "      print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(API_Link, pages=1):\n",
    "    with requests.get(API_Link + '&page=' + str(pages)) as response:\n",
    "        page_soup = soup(response.content, 'lxml')\n",
    "        return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for tag in topic_tags:\n",
    "      response = get_response(API_Link + tag, pages=str(1))\n",
    "\n",
    "      #parse\n",
    "      j_response = json.loads(response.text)\n",
    "      responses = responses + j_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'config', 'children', 'componentGroups'])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['excerpt', 'permalink', 'teaseCTA', 'title', 'topic', 'postDate', 'postFormat', 'topicLink', 'themeName', 'showImage', 'sponsorTagline', 'sponsorUrl', 'hideFeaturedImage', 'hideFooter', 'hideHeader', 'bodyLayout', 'storyColorTheme', 'headlineTextColor', 'headerBgColor', 'squareLeadImage', 'summaryBullets', 'subtopic', 'mitNewsTopic', 'mitNewsSubtopic', 'podcastLength', 'customEyebrow', 'byline', 'syndicate', 'hidePostContentAds', 'color'])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[1]['config'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, url, date, summary]\n",
       "Index: []"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial empty df\n",
    "collected_df = pd.DataFrame(columns=['title', 'url', 'date', 'summary'])\n",
    "collected_df.loc[collected_df.index, :] = ['x', 'url', 'date', 'summary']\n",
    "\n",
    "collected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_keys = ['title', 'permalink', 'postDate', 'excerpt']\n",
    "\n",
    "entries = []\n",
    "entries.append(['title', 'url', 'date', 'summary'])\n",
    "\n",
    "for item in responses:\n",
    "      entry = [item['config'][key] for key in needed_keys]\n",
    "      entries.append(entry)\n",
    "      #collected_df.loc[collected_df.index.max() + 1, :] = entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(entries[1:],columns=entries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_from_url(x):\n",
    "\n",
    "      pat = r\"(20[0-2][0-9]([-_/]?)[0-9]{2}(?:\\2[0-9]{2})?)\"\n",
    "      dates = re.compile(pat)\n",
    "\n",
    "      res = dates.search(x)\n",
    "\n",
    "      res = datetime_parse(res[0])\n",
    "  \n",
    "      return res[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n",
      "Successfully parsed with format: %Y %m %d\n"
     ]
    }
   ],
   "source": [
    "new_df['date'] = new_df['url'].apply(lambda x: date_from_url(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_text = []\n",
    "\n",
    "relevant_text.append(['title', 'url', 'date', 'summary', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.technologyreview.com/2022/04/22/1050394/artificial-intelligence-for-the-people/\n",
      "Search term found: Google\n",
      "Fetching: https://www.technologyreview.com/2022/04/21/1050381/the-gig-workers-fighting-back-against-the-algorithms/\n",
      "Fetching: https://www.technologyreview.com/2022/04/20/1050670/download-ai-capitalizes-catastrophe-bitcoin-cities-central-america-crypto/\n",
      "Fetching: https://www.technologyreview.com/2022/04/20/1050392/ai-industry-appen-scale-data-labels/\n",
      "Search term found: Google\n",
      "Fetching: https://www.technologyreview.com/2022/04/19/1049592/artificial-intelligence-colonialism/\n",
      "Search term found: Google\n",
      "Fetching: https://www.technologyreview.com/2022/04/19/1049378/ai-inequality-problem/\n",
      "Search term found: Google\n",
      "Fetching: https://www.technologyreview.com/2022/04/21/1050815/hackers-target-critical-infrastructure-pwn2own/\n",
      "Fetching: https://www.technologyreview.com/2022/04/21/1050747/cybercriminals-zero-day-hacks/\n",
      "Fetching: https://www.technologyreview.com/2022/04/21/1050788/the-changing-economics-of-open-source/\n",
      "Fetching: https://www.technologyreview.com/2022/04/21/1049824/flexibility-is-key-when-navigating-the-future-of-6g/\n"
     ]
    }
   ],
   "source": [
    "for index, row in new_df.iterrows():\n",
    "      if row.date >= today - timedelta(days=7):\n",
    "            print('Fetching: ' + row.url)\n",
    "            response = requests.post(row.url, headers=header)\n",
    "\n",
    "            html = soup(response.text, 'lxml')\n",
    "\n",
    "            objects = html.find_all('div', id=\"content--body\")\n",
    "            article_text = []\n",
    "            for obj in objects:\n",
    "                  paragraph_text = obj.find_all('p')\n",
    "                  for p in paragraph_text:\n",
    "                        article_text.append(p.text)\n",
    "                  #print(obj.text)\n",
    "            article_text = ' '.join(article_text)\n",
    "\n",
    "            for word in search_terms['search_term']:\n",
    "                  if word in article_text:\n",
    "                        save = list(row)\n",
    "                        save.append(article_text)\n",
    "                        relevant_text.append(save)                  \n",
    "                        print('Search term found: ' +  word)\n",
    "                        break\n",
    "            else: \n",
    "                  pass\n",
    "\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = pd.DataFrame(relevant_text[1:],columns=relevant_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: add if file clause\n",
    "old_df = pd.read_csv(DATA_PATH + 'synced_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([new_df, old_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If timing for collection and timedelta etc are good, should not be needed, but good to be sure\n",
    "# If you instead have date issues, you should do this in the collect loop to eliminate unneeeded collectio\n",
    "# e.g. if url in list(old_df.url): pass\n",
    "\n",
    "combined_df.drop_duplicates(subset='url', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(DATA_PATH + 'mit_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currently not working well - only 4 returns due to dynamic scrolling\n",
    "I can solve with Selenium, but trying to avoid using it\n",
    "\n",
    "Currently working on a requests version, but convoluated urls and html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "#pn = pagination\n",
    "# when pn exceeds pages the html is quite short and maybe has a tag in it\n",
    "\n",
    "url = \"https://spectrum.ieee.org/res/load_more_posts/data.js?site_id=20265424&node_id=/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/otherwise/choose/otherwise/element_wrapper-&resource_id=search_deepfake&path_params={}&formats=html&q=deepfake&rm_lazy_load=1&pn=5&pn_strategy=\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://spectrum.ieee.org/search/?q=Google&order=newest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "print(final_url)\n",
    "ua = UserAgent(verify_ssl=False, cache=False)\n",
    "\n",
    "user_agent = ua.random\n",
    "header = {'User-Agent': user_agent}\n",
    "\n",
    "response = requests.post(url, headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><p>{\"path_params\": {}, \"node_id\": \"/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/otherwise/choose/otherwise/element_wrapper-\", \"exclude_post_ids\": [], \"search_phrase\": \"deepfake\", \"device\": \"desktop\", \"posts_html\": \"</p><div class='\\\"\\\"' id='\\\"sSearch_0_0_11_0_0_7_0_1_2_0_1_0\\\"'><div class='\\\"mb-2' id='\\\"sSearch_0_0_11_0_0_7_0_1_2_0_1_0_0\\\"' infinite=\"\" story_teaser=\"\"><div class='\\\"posts-custom' clearfix=\"\" data-attr-all_element_order='\\\"all_sections,headline,subheadline,custom_field_PodcastEpisode,custom_field_PodcastVideoTime,date,time_to_read,like_button,custom_field_TimeofRead,tags,primary_tag,section,community_name,photo_caption,snark_line,author,post_shares,follow_button,community_comments,source_link,body,badges_sponsored,photo_credit,main_author,page_views,date_modified,custom_field_contact-person-phone,custom_field_event-performer,custom_field_event-street-address,custom_field_job-type,custom_field_event-postal-code,custom_field_contact-person-title,custom_field_event-start,custom_field_company_name,custom_field_event-end,custom_field_hiring-organization,custom_field_event-price,custom_field_job-tags,custom_field_event-city,custom_field_contact_email,custom_field_posted-on-date,custom_field_job-title,custom_field_job-location,custom_field_event-region,custom_field_event-currency,custom_field_base-salary,custom_field_event-country,custom_field_contact_person,custom_field_expire-date,custom_field_ContentType,custom_field_CustomFieldTestEG,custom_field_Issue,custom_field_FullwidthCardColor,custom_field_FullwidthCardPosition,product_prices,collection_button,badges,site_field_Twitter,site_field_Facebook,site_field_Instagram,site_field_LinkedIn,product_buy_link,custom_field_Editor,product_vendor,words_count,site_field_first_name,site_field_last_name,site_field_member_badges,site_field_grade,custom_field_due_date,custom_field_FeaturePostImgPosition,custom_field_pdf_version,custom_field_access,custom_field_publication_date,custom_field_FeaturePostColor,custom_field_fullwidthbackgroundposition,custom_field_disable_comments,custom_field_lightbox_img_shortcode_ids,custom_field_copy_editor\\\"\\n' data-attr-allow_duplicates='\\\"true\\\"\\n' data-attr-data-rm-advanced='\\\"true\\\"\\n' data-attr-element_classes='\\\"story_teaser' data-attr-format='\\\"posts-custom\\\"\\n' data-attr-from_regular_sections_other_than='\\\"internal/announcement-bar\\\"\\n' data-attr-layout_all_date_format='\\\"%d' data-attr-layout_all_date_full_format='\\\"false\\\"\\n' data-attr-layout_all_image_crop='\\\"original\\\"\\n' data-attr-layout_all_sections='\\\"right\\\"\\n' data-attr-layout_custom_field_podcastepisode='\\\"right\\\"\\n' data-attr-layout_custom_field_podcastvideotime='\\\"right\\\"\\n' data-attr-layout_date='\\\"right\\\"\\n' data-attr-layout_headline='\\\"right\\\"\\n' data-attr-layout_image_column_width='\\\"25\\\"\\n' data-attr-layout_like_button='\\\"right\\\"\\n' data-attr-layout_quality='\\\"4\\\"' data-attr-layout_subheadline='\\\"right\\\"\\n' data-attr-layout_time_to_read='\\\"right\\\"\\n' data-attr-limit='\\\"4\\\"\\n' data-attr-node_id='\\\"/root/blocks/block[search]/abtests/abtest[1]/row/column[1]/choose/otherwise/choose/otherwise/element_wrapper/posts-\\\"\\n' data-attr-order='\\\"relevance\\\"\\n' data-attr-phrase='\\\"deepfake\\\"\\n' data-attr-posts_id='\\\"sSearch_0_0_11_0_0_7_0_1_2_0_1_0_0\\\"\\n' data-attr-section_url='\\\"\\\"\\n' data-attr-source='\\\"current_page\\\"\\n' data-attr-source_id='\\\"current_page\\\"\\n' data-attr-source_site='\\\"parent_site\\\"\\n' data-attr-source_url='\\\"current_page\\\"\\n' data-attr-use_tag_image_for_lead_media='\\\"true\\\"\\n' data-attr-v='\\\"2\\\"\\n' data-attr-without_current='\\\"true\\\"\\n' data-block='\\\"section_1\\\"\\n' data-format='\\\"posts-custom\\\"\\n' data-has-more='\\\"true\\\"\\n\\n' data-is-reordable='\\\"false\\\"\\n' data-section-id='\\\"\\\"\\n' data-source='\\\"current_page\\\"\\n' data-source-type='\\\"page\\\"\\n' data-source-unique='\\\"false\\\"\\n' data-using-stickers='\\\"false\\\"\\n' infinite=\"\" posts-custom-section=\"\" section-holder=\"\"><div class='\\\"posts-wrapper' clearfix=\"\">\\n            \\n\\n                \\n                <div class='\\\"widget' data-category='\\\"Computing\\\"' elid='\\\"2650278855\\\"\\n' post-section--topic=\"\" tag-ai=\"\" tag-deepfakes=\"\" tag-image-analysis=\"\" tag-journal-watch=\"\" tag-machine-learning=\"\" tag-neural-networks=\"\" tag-security=\"\" tag-software=\"\"><article class='\\\"clearfix' data-is-frozen='\\\"false\\\"' elid='\\\"2650278855\\\"' page-article=\"\" post-2650278855=\"\" quality-hd=\"\" sm-mb-1=\"\">\\n    <div class='\\\"row' px10=\"\"><div class='\\\"col' id='\\\"col-center\\\"' sm-mb-1=\"\" style='\\\"width:25.0%;\\\"'><div class='\\\"widget__head\\\"'><a algorithm=\"\" aria-label='\\\"A' deepfake=\"\" detect=\"\" href='\\\"https://spectrum.ieee.org/a-twotrack-algorithm-to-detect-deepfake-images\\\"' images=\"\" to=\"\" two-track=\"\"><img a=\"\" alt='\\\"Image' aria-label='\\\"Image' bird=\"\" branch=\"\" class='\\\"rm-lazyloadable-image\\\"\\n' cover=\"\" data-runner-src='\\\"https://spectrum.ieee.org/media-library/image-of-a-bird-gripping-a-branch.jpg?id=25589294&amp;width=1200&amp;height=740\\\"\\n' gripping=\"\" height='\\\"568\\\"\\n' of=\"\" role='\\\"img\\\"\\n' src='\\\"data:image/svg+xml,%3Csvg' style='\\\"object-fit:' type='\\\"lazy-image\\\"\\n' viewbox=\"'0\" width='\\\"920\\\"\\n' xmlns=\"'http://www.w3.org/2000/svg'\"/></a>\\n                        \\n                </div></div><div class='\\\"col\\\"' id='\\\"col-right\\\"' style='\\\"width:75.0%;\\\"'>\\n                <div class='\\\"widget__body' clearfix=\"\" sm-mt-1=\"\">\\n               \\n        \\n               \\n        \\n               \\n        \\n               \\n        <div class='\\\"all-related-sections\\\"'>\\n                <a href='\\\"https://spectrum.ieee.org/topic/computing/\\\"'>Computing</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/topic/\\\"'>Topic</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/type/\\\"'>Type</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/type/news/\\\"'>News</a>\\n            </div><h2 class='\\\"widget__headline' h2=\"\">\\n    <a algorithm=\"\" aria-label='\\\"A' class='\\\"widget__headline-text' data-type='\\\"text\\\"' deepfake=\"\" detect=\"\" href='\\\"https://spectrum.ieee.org/a-twotrack-algorithm-to-detect-deepfake-images\\\"' images=\"\" to=\"\" two-track=\"\">\\n        A Two-Track Algorithm To Detect Deepfake Images\\n    </a>\\n</h2>\\n<div class='\\\"widget__subheadline\\\"'>\\n    <h3 class='\\\"widget__subheadline-text' data-type='\\\"text\\\"' h3=\"\">A neural-network-based tool can spot image manipulation at the level of single-pixels</h3>\\n</div><div class='\\\"social-date\\\"'>\\n        <span class='\\\"social-date__text\\\"'>29 Jul 2019</span>\\n    </div><div class='\\\"time-to-read\\\"'>3 min read</div><div class='\\\"like-button\\\"' data-post-likes='\\\"0\\\"'></div></div>\\n            </div></div>\\n    \\n    \\n<script id='\\\"post-context-2650278855\\\"' type='\\\"application/json\\\"'>\\n    {\\\"customDimensions\\\": {\\\"5\\\":\\\"Mark Anderson\\\",\\\"6\\\":\\\"computing\\\",\\\"7\\\":\\\"software, security, deepfakes, neural networks, AI, Journal Watch, image analysis, machine learning, ~rmsc:rebelmouse-image:25589294\\\",\\\"8\\\":\\\"07/29/2019\\\",\\\"10\\\":\\\"software\\\",\\\"11\\\":2650278855}, \\\"post\\\": {\\\"split_testing\\\": {}, \\\"providerId\\\": 20, \\\"sections\\\": [0, 497728257, 544169516, 539617903, 544169523, 559931002], \\\"buckets\\\": [], \\\"authors\\\": [21079057]} }\\n</script>\\n</article>\\n                </div>\\n            \\n\\n                \\n                <div class='\\\"widget' data-category='\\\"Careers\\\"' elid='\\\"2650279866\\\"\\n' post-section--topic=\"\" tag-careers=\"\" tag-engineering-careers=\"\" tag-ethics=\"\" tag-innovation=\"\" tag-military=\"\" tag-tech-careers=\"\"><article class='\\\"clearfix' data-is-frozen='\\\"false\\\"' elid='\\\"2650279866\\\"' page-article=\"\" post-2650279866=\"\" quality-hd=\"\" sm-mb-1=\"\">\\n    <div class='\\\"row' px10=\"\"><div class='\\\"col' id='\\\"col-center\\\"' sm-mb-1=\"\" style='\\\"width:25.0%;\\\"'><div class='\\\"widget__head\\\"'><a activist=\"\" aria-label='\\\"How' engineering=\"\" href='\\\"https://spectrum.ieee.org/how-to-practice-activist-engineering\\\"' practice=\"\" to=\"\"><img a=\"\" alt='\\\"Illustration' and=\"\" aria-label='\\\"Illustration' brain=\"\" circuits=\"\" class='\\\"rm-lazyloadable-image\\\"\\n' connecting=\"\" cover=\"\" data-runner-src='\\\"https://spectrum.ieee.org/media-library/illustration-of-a-brain-and-heart-each-made-of-circuits-connecting.jpg?id=25591281&amp;width=1200&amp;height=630\\\"\\n' each=\"\" heart=\"\" height='\\\"630\\\"\\n' made=\"\" of=\"\" role='\\\"img\\\"\\n' src='\\\"data:image/svg+xml,%3Csvg' style='\\\"object-fit:' type='\\\"lazy-image\\\"\\n' viewbox=\"'0\" width='\\\"1200\\\"\\n' xmlns=\"'http://www.w3.org/2000/svg'\"/></a>\\n                        \\n                </div></div><div class='\\\"col\\\"' id='\\\"col-right\\\"' style='\\\"width:75.0%;\\\"'>\\n                <div class='\\\"widget__body' clearfix=\"\" sm-mt-1=\"\">\\n               \\n        \\n               \\n        \\n               \\n        \\n               \\n        <div class='\\\"all-related-sections\\\"'>\\n                <a href='\\\"https://spectrum.ieee.org/topic/careers/\\\"'>Careers</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/topic/\\\"'>Topic</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/type/\\\"'>Type</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/type/opinion/\\\"'>Opinion</a>\\n            </div><h2 class='\\\"widget__headline' h2=\"\">\\n    <a activist=\"\" aria-label='\\\"How' class='\\\"widget__headline-text' data-type='\\\"text\\\"' engineering=\"\" href='\\\"https://spectrum.ieee.org/how-to-practice-activist-engineering\\\"' practice=\"\" to=\"\">\\n        How to Practice Activist Engineering\\n    </a>\\n</h2>\\n<div class='\\\"widget__subheadline\\\"'>\\n    <h3 class='\\\"widget__subheadline-text' data-type='\\\"text\\\"' h3=\"\">What can engineers do when their work doesn't align with their personal values of creating a better world?</h3>\\n</div><div class='\\\"social-date\\\"'>\\n        <span class='\\\"social-date__text\\\"'>03 Apr 2020</span>\\n    </div><div class='\\\"time-to-read\\\"'>4 min read</div><div class='\\\"like-button\\\"' data-post-likes='\\\"0\\\"'></div></div>\\n            </div></div>\\n    \\n    \\n<script id='\\\"post-context-2650279866\\\"' type='\\\"application/json\\\"'>\\n    {\\\"customDimensions\\\": {\\\"5\\\":\\\"Darshan M.A. Karwat\\\",\\\"6\\\":\\\"careers\\\",\\\"7\\\":\\\"tech careers, innovation, engineering careers, careers, military, ethics\\\",\\\"8\\\":\\\"04/03/2020\\\",\\\"10\\\":\\\"tech careers\\\",\\\"11\\\":2650279866}, \\\"post\\\": {\\\"split_testing\\\": {}, \\\"providerId\\\": 20, \\\"sections\\\": [0, 497728257, 539628627, 544169516, 544169525], \\\"buckets\\\": [], \\\"authors\\\": [21081568]} }\\n</script>\\n</article>\\n                </div>\\n            \\n\\n                \\n                <div class='\\\"widget' data-category='\\\"Artificial' elid='\\\"2650279614\\\"\\n' intelligence=\"\" post-section--topic=\"\" tag-adversarial-attacks=\"\" tag-ai=\"\" tag-embedded-ai=\"\" tag-guest-articles=\"\" tag-military-robots=\"\" tag-robot-ai=\"\"><article class='\\\"clearfix' data-is-frozen='\\\"false\\\"' elid='\\\"2650279614\\\"' page-article=\"\" post-2650279614=\"\" quality-hd=\"\" sm-mb-1=\"\">\\n    <div class='\\\"row' px10=\"\"><div class='\\\"col' id='\\\"col-center\\\"' sm-mb-1=\"\" style='\\\"width:25.0%;\\\"'><div class='\\\"widget__head\\\"'><a adversarial=\"\" ai=\"\" aria-label='\\\"How' attacks=\"\" could=\"\" destabilize=\"\" href='\\\"https://spectrum.ieee.org/adversarial-attacks-and-ai-systems\\\"' military=\"\" systems=\"\"><img a=\"\" alt='\\\"Illustration' aria-label='\\\"Illustration' class='\\\"rm-lazyloadable-image\\\"\\n' connecting=\"\" cover=\"\" data-runner-src='\\\"https://spectrum.ieee.org/media-library/illustration-of-connecting-networks-forming-a-human-head-shape.jpg?id=25590812&amp;width=1200&amp;height=900\\\"\\n' forming=\"\" head=\"\" height='\\\"930\\\"\\n' human=\"\" networks=\"\" of=\"\" role='\\\"img\\\"\\n' shape=\"\" src='\\\"data:image/svg+xml,%3Csvg' style='\\\"object-fit:' type='\\\"lazy-image\\\"\\n' viewbox=\"'0\" width='\\\"1240\\\"\\n' xmlns=\"'http://www.w3.org/2000/svg'\"/></a>\\n                        \\n                </div></div><div class='\\\"col\\\"' id='\\\"col-right\\\"' style='\\\"width:75.0%;\\\"'>\\n                <div class='\\\"widget__body' clearfix=\"\" sm-mt-1=\"\">\\n               \\n        \\n               \\n        \\n               \\n        \\n               \\n        <div class='\\\"all-related-sections\\\"'>\\n                <a href='\\\"https://spectrum.ieee.org/topic/artificial-intelligence/\\\"'>Artificial Intelligence</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/topic/\\\"'>Topic</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/type/\\\"'>Type</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/type/news/\\\"'>News</a>\\n            </div><h2 class='\\\"widget__headline' h2=\"\">\\n    <a adversarial=\"\" ai=\"\" aria-label='\\\"How' attacks=\"\" class='\\\"widget__headline-text' could=\"\" data-type='\\\"text\\\"' destabilize=\"\" href='\\\"https://spectrum.ieee.org/adversarial-attacks-and-ai-systems\\\"' military=\"\" systems=\"\">\\n        How Adversarial Attacks Could Destabilize Military AI Systems\\n    </a>\\n</h2>\\n<div class='\\\"widget__subheadline\\\"'>\\n    <h3 class='\\\"widget__subheadline-text' data-type='\\\"text\\\"' h3=\"\">Adversarial attacks threaten the safety of AI and robotic technologies. Can we stop them?</h3>\\n</div><div class='\\\"social-date\\\"'>\\n        <span class='\\\"social-date__text\\\"'>26 Feb 2020</span>\\n    </div><div class='\\\"time-to-read\\\"'>6 min read</div><div class='\\\"like-button\\\"' data-post-likes='\\\"0\\\"'></div></div>\\n            </div></div>\\n    \\n    \\n<script id='\\\"post-context-2650279614\\\"' type='\\\"application/json\\\"'>\\n    {\\\"customDimensions\\\": {\\\"5\\\":\\\"David Danks\\\",\\\"6\\\":\\\"artificial-intelligence\\\",\\\"7\\\":\\\"embedded ai, military robots, robot ai, guest articles, adversarial attacks, AI\\\",\\\"8\\\":\\\"02/26/2020\\\",\\\"10\\\":\\\"embedded ai\\\",\\\"11\\\":2650279614}, \\\"post\\\": {\\\"split_testing\\\": {}, \\\"providerId\\\": 20, \\\"sections\\\": [0, 497728257, 497728259, 544169516, 544169523], \\\"buckets\\\": [], \\\"authors\\\": [21081539]} }\\n</script>\\n</article>\\n                </div>\\n            \\n\\n                \\n                <div class='\\\"widget' data-category='\\\"Artificial' elid='\\\"2650279175\\\"\\n' intelligence=\"\" post-section--topic=\"\" tag-ai=\"\" tag-history=\"\" tag-history-of-natural-language-processing=\"\" tag-machine-learning=\"\" tag-natural-language-processing=\"\" tag-nlp=\"\" tag-openai=\"\" tag-software=\"\"><article class='\\\"clearfix' data-is-frozen='\\\"false\\\"' elid='\\\"2650279175\\\"' page-article=\"\" post-2650279175=\"\" quality-hd=\"\" sm-mb-1=\"\">\\n    <div class='\\\"row' px10=\"\"><div class='\\\"col' id='\\\"col-center\\\"' sm-mb-1=\"\" style='\\\"width:25.0%;\\\"'><div class='\\\"widget__head\\\"'><a a=\"\" aria-label='\\\"For' centuries=\"\" could=\"\" dreamed=\"\" href='\\\"https://spectrum.ieee.org/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one\\\"' language.=\"\" machine=\"\" made=\"\" of=\"\" one=\"\" openai=\"\" people=\"\" produce=\"\" that=\"\" then=\"\"><img a=\"\" ai=\"\" alt='\\\"Greg' and=\"\" aria-label='\\\"Greg' background=\"\" brockman=\"\" chart=\"\" class='\\\"rm-lazyloadable-image\\\"\\n' comprised=\"\" cover=\"\" data-runner-src='\\\"https://spectrum.ieee.org/media-library/greg-brockman-ilya-sutskever-from-open-ai-in-front-of-a-background-comprised-of-a-generic-language-model-chart-gpt-2-screenshot-and-reddit-screenshots.jpg?id=25589914&amp;width=1200&amp;height=900\\\"\\n' from=\"\" front=\"\" generic=\"\" gpt-2=\"\" height='\\\"930\\\"\\n' ilya=\"\" in=\"\" language=\"\" model=\"\" of=\"\" open=\"\" reddit=\"\" role='\\\"img\\\"\\n' screenshot=\"\" screenshots.=\"\" src='\\\"data:image/svg+xml,%3Csvg' style='\\\"object-fit:' sutskever=\"\" type='\\\"lazy-image\\\"\\n' viewbox=\"'0\" width='\\\"1240\\\"\\n' xmlns=\"'http://www.w3.org/2000/svg'\"/></a>\\n                        \\n                </div></div><div class='\\\"col\\\"' id='\\\"col-right\\\"' style='\\\"width:75.0%;\\\"'>\\n                <div class='\\\"widget__body' clearfix=\"\" sm-mt-1=\"\">\\n               \\n        \\n               \\n        \\n               \\n        \\n               \\n        <div class='\\\"all-related-sections\\\"'>\\n                <a href='\\\"https://spectrum.ieee.org/topic/artificial-intelligence/\\\"'>Artificial Intelligence</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/topic/\\\"'>Topic</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/type/\\\"'>Type</a>\\n            \\n                <a href='\\\"https://spectrum.ieee.org/type/article/\\\"'>Article</a>\\n            </div><h2 class='\\\"widget__headline' h2=\"\">\\n    <a a=\"\" aria-label='\\\"For' centuries=\"\" class='\\\"widget__headline-text' could=\"\" data-type='\\\"text\\\"' dreamed=\"\" href='\\\"https://spectrum.ieee.org/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one\\\"' language.=\"\" machine=\"\" made=\"\" of=\"\" one=\"\" openai=\"\" people=\"\" produce=\"\" that=\"\" then=\"\">\\n        For Centuries, People Dreamed of a Machine That Could Produce Language. Then OpenAI Made One\\n    </a>\\n</h2>\\n<div class='\\\"widget__subheadline\\\"'>\\n    <h3 class='\\\"widget__subheadline-text' data-type='\\\"text\\\"' h3=\"\">OpenAI\\u2019s GPT-2 program churns out natural language that\\u2019s remarkably coherent\\u2014and that\\u2019s a problem</h3>\\n</div><div class='\\\"social-date\\\"'>\\n        <span class='\\\"social-date__text\\\"'>02 Dec 2019</span>\\n    </div><div class='\\\"time-to-read\\\"'>4 min read</div><div class='\\\"like-button\\\"' data-post-likes='\\\"1\\\"'></div></div>\\n            </div></div>\\n    \\n    \\n<script id='\\\"post-context-2650279175\\\"' type='\\\"application/json\\\"'>\\n    {\\\"customDimensions\\\": {\\\"5\\\":\\\"Oscar Schwartz\\\",\\\"6\\\":\\\"artificial-intelligence\\\",\\\"7\\\":\\\"machine learning, software, history of natural language processing, NLP, natural language processing, AI, OpenAI, history\\\",\\\"8\\\":\\\"12/02/2019\\\",\\\"10\\\":\\\"machine learning\\\",\\\"11\\\":2650279175}, \\\"post\\\": {\\\"split_testing\\\": {}, \\\"providerId\\\": 20, \\\"sections\\\": [0, 497728257, 497728259, 544169516, 544175596], \\\"buckets\\\": [], \\\"authors\\\": [21081361]} }\\n</script>\\n</article>\\n                </div>\\n    </div></div>\\n    \\n\\n    \\n\\n    \\n    </div><div class='\\\"search-ads\\\"' id='\\\"sSearch_0_0_11_0_0_7_0_1_2_0_1_0_1\\\"'><!-- User Code --><div class='\\\"infinite-leaderboard\\\"'></div><!-- End User Code --></div></div>\", \"css\": \"\\n\\n#sSearch_0_0_11_0_0_7_0_1_2_0_1_0_1 {padding:32px 0;width:calc(100% - 64px);z-index:2;background:#ecece8;position:relative;margin:auto;text-align:center;}\"}</body></html>"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "4\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "\n",
      "\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "4\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "\n",
      "\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "4\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "\n",
      "\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "4\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "\n",
      "\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "1\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "1\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "0\n",
      "0\n",
      "1\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "1\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "0\n",
      "1\n",
      "Commercialization of AI-assisted dubbing for TV and films hints at bigger possibilities with digitally captured acting performances\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "1\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "0\n",
      "0\n",
      "1\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "1\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "0\n",
      "1\n",
      "Facebook researchers release a dataset intended to help machine learning developers test algorithms for bias\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "objects = html.find_all('div')#,  class_='section_column')#elid=True)#class_= 'clearfix')\n",
    "print(len(objects))\n",
    "\n",
    "for obj in objects:\n",
    "      items = obj.find_all('h3')#, class_ = 'widget__head')#, id='col-right')\n",
    "      print(len(items))\n",
    "      for i in items:\n",
    "\n",
    "            print(i.text)\n",
    "            # title_list.append(i.h2.text)\n",
    "\n",
    "            # print(i.h2.a['href'])\n",
    "            # url_list.append(i.h2.a['href'])\n",
    "\n",
    "            # date = i.div.span\n",
    "            # print(date.text)\n",
    "            # try: \n",
    "            #       #not working well\n",
    "            #       d_parsed = datetime_parse(date.text)\n",
    "            #       date_list.append(d_parsed[0])\n",
    "            # except:\n",
    "            #       date_list.append('None')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url ='https://spectrum.ieee.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = 'https://spectrum.ieee.org/search/?q='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_url = 'https://spectrum.ieee.org/topic/artificial-intelligence/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query connector\n",
    "C = \"&\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_criteria = \"order=newest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_criteria = \"topic=artificial-intelligence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 9, 46, 7, 425081)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialized above\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 4, 22)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 3, 23)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_to = today.date() - timedelta(days=30)\n",
    "back_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeepMind', 'Google']"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms['search_term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://spectrum.ieee.org/search/?q=DeepMind&order=newest\n",
      "https://spectrum.ieee.org/search/?q=Google&order=newest\n"
     ]
    }
   ],
   "source": [
    "for term in search_terms['search_term']:\n",
    "      final_url = query_url + term + C + sort_criteria\n",
    "      print(final_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://spectrum.ieee.org/search/?q=Google&order=newest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "print(final_url)\n",
    "ua = UserAgent(verify_ssl=False, cache=False)\n",
    "\n",
    "user_agent = ua.random\n",
    "header = {'User-Agent': user_agent}\n",
    "\n",
    "response = requests.post(final_url, headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "date_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n",
      "\n",
      "        Video Friday: DALL-E 2\n",
      "    \n",
      "\n",
      "https://spectrum.ieee.org/video-friday-dall-e-2\n",
      "08 Apr 2022\n",
      "Successfully parsed with format: %m %b %Y\n",
      "\n",
      "\n",
      "        How a Parachute Accident Helped Jump-start Augmented Reality\n",
      "    \n",
      "\n",
      "https://spectrum.ieee.org/history-of-augmented-reality\n",
      "07 Apr 2022\n",
      "Successfully parsed with format: %m %b %Y\n",
      "\n",
      "\n",
      "        Volunteers Scramble to Preserve Ukraine’s Digital Culture\n",
      "    \n",
      "\n",
      "https://spectrum.ieee.org/archiving-ukraine-culture\n",
      "06 Apr 2022\n",
      "Successfully parsed with format: %m %b %Y\n",
      "\n",
      "\n",
      "        How the Wayback Machine Is Saving Digital Ukraine\n",
      "    \n",
      "\n",
      "https://spectrum.ieee.org/internet-archive-ukraine\n",
      "06 Apr 2022\n",
      "Successfully parsed with format: %m %b %Y\n"
     ]
    }
   ],
   "source": [
    "# Works some at least but only fetches top 4 due to page scroll\n",
    "\n",
    "objects = html.find_all('div',  class_='section_column')#elid=True)#class_= 'clearfix')\n",
    "\n",
    "for obj in objects:\n",
    "      items = obj.find_all('div', id='col-right')\n",
    "      print(len(items))\n",
    "      for i in items:\n",
    "\n",
    "            print(i.h2.text)\n",
    "            title_list.append(i.h2.text)\n",
    "\n",
    "            print(i.h2.a['href'])\n",
    "            url_list.append(i.h2.a['href'])\n",
    "\n",
    "            date = i.div.span\n",
    "            print(date.text)\n",
    "            try: \n",
    "                  #not working well\n",
    "                  d_parsed = datetime_parse(date.text)\n",
    "                  date_list.append(d_parsed[0])\n",
    "            except:\n",
    "                  date_list.append('None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n        Video Friday: DALL-E 2\\n    \\n</td>\n",
       "      <td>https://spectrum.ieee.org/video-friday-dall-e-2</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n        How a Parachute Accident Helped Ju...</td>\n",
       "      <td>https://spectrum.ieee.org/history-of-augmented...</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n        Volunteers Scramble to Preserve Uk...</td>\n",
       "      <td>https://spectrum.ieee.org/archiving-ukraine-cu...</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n        How the Wayback Machine Is Saving ...</td>\n",
       "      <td>https://spectrum.ieee.org/internet-archive-ukr...</td>\n",
       "      <td>2022-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \\n\\n        Video Friday: DALL-E 2\\n    \\n   \n",
       "1  \\n\\n        How a Parachute Accident Helped Ju...   \n",
       "2  \\n\\n        Volunteers Scramble to Preserve Uk...   \n",
       "3  \\n\\n        How the Wayback Machine Is Saving ...   \n",
       "\n",
       "                                                 url       date  \n",
       "0    https://spectrum.ieee.org/video-friday-dall-e-2 2022-04-01  \n",
       "1  https://spectrum.ieee.org/history-of-augmented... 2022-04-01  \n",
       "2  https://spectrum.ieee.org/archiving-ukraine-cu... 2022-04-01  \n",
       "3  https://spectrum.ieee.org/internet-archive-ukr... 2022-04-01  "
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collected3 = pd.DataFrame(list(zip(title_list, url_list, date_list)), \n",
    "            columns=['title', 'url', 'date',])\n",
    "\n",
    "df_collected3.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to save to\n",
    "relevant_text = []\n",
    "\n",
    "#preping list fo dictionary conversion\n",
    "relevant_text.append(['title', 'url', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 9, 46, 7, 425081)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_collected3.iterrows():\n",
    "      if row.date >= today - timedelta(days=30):\n",
    "            print('Fetching: ' + row.url)\n",
    "            response = requests.post(row.url) #headers=header)\n",
    "\n",
    "            html = soup(response.text, 'lxml')\n",
    "\n",
    "            objects = html.find_all('div', id=\"col-center\")\n",
    "            article_text = []\n",
    "            for obj in objects:\n",
    "                  paragraph_text = obj.find_all('p')\n",
    "                  for p in paragraph_text:\n",
    "                        article_text.append(p.text)\n",
    "                  #print(obj.text)\n",
    "            article_text = ' '.join(article_text)\n",
    "   \n",
    "            for word in search_terms['search_term']:\n",
    "                  if word in article_text:\n",
    "                        save = list(row)\n",
    "                        save.append(article_text)\n",
    "                        relevant_text.append(save)                  \n",
    "                        print('Search term found: ' +  word)\n",
    "                        break\n",
    "            else: \n",
    "                  pass\n",
    "\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  urllib.request import urlopen\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_1 = 'https://www.cna.org/centers/cna/sppp/rsp/russia-ai-archive#newsletters'\n",
    "base_url_2  = \"https://www.cna.org/centers/cna/cip/china/china-ai-newsletters/issue-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PDF \n",
    "# clean up PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 22, 15, 37, 18, 711935)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 3, 23)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_to = today.date() - timedelta(days=30)\n",
    "back_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loop over base_url_1 and_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/home/jesselehrke/Documents/GitHub/disinfo_radar_development/dri_venv/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "ua = UserAgent(verify_ssl=False, cache=False)\n",
    "\n",
    "user_agent = ua.random\n",
    "header = {'User-Agent': user_agent}\n",
    "\n",
    "response = requests.get(base_url_2, headers=header)\n",
    "\n",
    "html = soup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "url_list = []\n",
    "date_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "13\n",
      "Issue 1: November 2, 2021 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-1.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 2: November 9, 2021 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-2.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 3: November 18, 2021 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-3.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 4: December 2, 2021 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-4.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 5: December 16, 2021 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-5.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 6: January 13, 2022 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-6.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 7: January 27, 2022 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-7.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 8: February 10, 2022 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-8.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 9: February 24, 2022 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-9.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 10: March 10, 2022 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-10.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 11: March 24, 2022 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-11.pdf\n",
      "Successfully parsed with format: %B %d %Y\n",
      "Issue 12: April 7, 2022 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-12.pdf\n",
      "Successfully parsed with format: %B %m %Y\n",
      "Issue 13: April 21, 2022 [ web | pdf ]\n",
      "/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-13.pdf\n",
      "Successfully parsed with format: %B %d %Y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "objects = html.find_all('div', id='newsletters')#, href=True)#elid=True)#class_= 'clearfix')\n",
    "print(len(objects))\n",
    "\n",
    "for obj in objects:\n",
    "      items = obj.find_all('li')#, href=True)\n",
    "      print(len(items))\n",
    "      for i in items:\n",
    "            print(i.text)\n",
    "            title_list.append(i.text)\n",
    "            urls = i.find_all('a', href=True)\n",
    "            print(urls[1]['href'])\n",
    "            url_list.append('https://www.cna.org' + urls[1]['href'])\n",
    "\n",
    "            date = i.text\n",
    "            try:\n",
    "                  d_parsed = datetime_parse(date)\n",
    "                  date_list.append(d_parsed[0])\n",
    "            except:\n",
    "                  date_list.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Issue 1: November 2, 2021 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Issue 2: November 9, 2021 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Issue 3: November 18, 2021 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Issue 4: December 2, 2021 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Issue 5: December 16, 2021 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2021-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Issue 6: January 13, 2022 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Issue 7: January 27, 2022 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Issue 8: February 10, 2022 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Issue 9: February 24, 2022 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Issue 10: March 10, 2022 [ web | pdf ]</td>\n",
       "      <td>https://www.cna.org/CNA_files/centers/CNA/CIP/...</td>\n",
       "      <td>2022-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  \\\n",
       "0   Issue 1: November 2, 2021 [ web | pdf ]   \n",
       "1   Issue 2: November 9, 2021 [ web | pdf ]   \n",
       "2  Issue 3: November 18, 2021 [ web | pdf ]   \n",
       "3   Issue 4: December 2, 2021 [ web | pdf ]   \n",
       "4  Issue 5: December 16, 2021 [ web | pdf ]   \n",
       "5   Issue 6: January 13, 2022 [ web | pdf ]   \n",
       "6   Issue 7: January 27, 2022 [ web | pdf ]   \n",
       "7  Issue 8: February 10, 2022 [ web | pdf ]   \n",
       "8  Issue 9: February 24, 2022 [ web | pdf ]   \n",
       "9    Issue 10: March 10, 2022 [ web | pdf ]   \n",
       "\n",
       "                                                 url       date  \n",
       "0  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-02-01  \n",
       "1  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-09-01  \n",
       "2  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-11-18  \n",
       "3  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-02-01  \n",
       "4  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2021-12-16  \n",
       "5  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-01-13  \n",
       "6  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-01-27  \n",
       "7  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-10-01  \n",
       "8  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-02-24  \n",
       "9  https://www.cna.org/CNA_files/centers/CNA/CIP/... 2022-10-01  "
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save collection to dataframe\n",
    "\n",
    "df_collected = pd.DataFrame(list(zip(title_list, url_list, date_list)), \n",
    "            columns=['title', 'url', 'date'])\n",
    "\n",
    "df_collected.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = 'CNA'\n",
    "pdf_dir = DATA_PATH + QUERY + 'pdfs'\n",
    "\n",
    "if not os.path.exists(pdf_dir): \n",
    "  os.makedirs(pdf_dir)\n",
    "\n",
    "have = set(os.listdir(pdf_dir))\n",
    "\n",
    "# Time out for requests\n",
    "timeout_secs = 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-8.pdf\n",
      "../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-8.pdf\n",
      "fetching https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-8.pdf into ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-8.pdf\n",
      "Fetching: https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-10.pdf\n",
      "../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-10.pdf\n",
      "fetching https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-10.pdf into ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-10.pdf\n",
      "Fetching: https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-12.pdf\n",
      "../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-12.pdf\n",
      "fetching https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-12.pdf into ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-12.pdf\n",
      "Fetching: https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-13.pdf\n",
      "../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-13.pdf\n",
      "fetching https://www.cna.org/CNA_files/centers/CNA/CIP/China/ai-newsletters/ChinaAI-Autonomy-Report-Issue-13.pdf into ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-13.pdf\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_collected.iterrows():\n",
    "      if row.date >= today - timedelta(days=14):\n",
    "\n",
    "            basename = row.url.split('/')[-1]\n",
    "            fname = os.path.join(pdf_dir, basename)\n",
    "            print(fname)\n",
    "\n",
    "            # try:\n",
    "            if not basename in have:\n",
    "                  print('fetching %s into %s' % (row.url, fname))\n",
    "                  req = urlopen(row.url, None, timeout_secs)\n",
    "                  with open(fname, 'wb') as fp:\n",
    "                        shutil.copyfileobj(req, fp)\n",
    "            else:\n",
    "                  print('%s exists, skipping' % (fname, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for a program and a folder\n",
    "if not shutil.which('pdftotext'): # needs Python 3.3+\n",
    "  print('ERROR: you don\\'t have pdftotext installed. Install it first before calling this script')\n",
    "  sys.exit()\n",
    "\n",
    "if not os.path.exists(DATA_PATH + QUERY + '_txt'):\n",
    "      os.makedirs(DATA_PATH + QUERY + '_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying paths\n",
    "txt_dir = DATA_PATH + QUERY + '_txt'\n",
    "pdf_dir = DATA_PATH + QUERY + 'pdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/6 skipping AI-and-autonomy-in-Russia-Issue-31.pdf.txt, already exists.\n",
      "1/6 skipping AI-and-Autonomy-in-Russia-Issue-29-January-10-2022.pdf.txt, already exists.\n",
      "2/6 pdftotext ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-8.pdf ../data/CNA_txt/ChinaAI-Autonomy-Report-Issue-8.pdf.txt\n",
      "3/6 pdftotext ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-10.pdf ../data/CNA_txt/ChinaAI-Autonomy-Report-Issue-10.pdf.txt\n",
      "4/6 pdftotext ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-12.pdf ../data/CNA_txt/ChinaAI-Autonomy-Report-Issue-12.pdf.txt\n",
      "5/6 pdftotext ../data/CNApdfs/ChinaAI-Autonomy-Report-Issue-13.pdf ../data/CNA_txt/ChinaAI-Autonomy-Report-Issue-13.pdf.txt\n"
     ]
    }
   ],
   "source": [
    "# Note- should make a function in utils and import\n",
    "\n",
    "have = set(os.listdir(txt_dir))\n",
    "files = os.listdir(pdf_dir)\n",
    "\n",
    "for i,f in enumerate(files):\n",
    "\n",
    "  txt_basename = f + '.txt'\n",
    "  \n",
    "  if txt_basename in have:\n",
    "    print('%d/%d skipping %s, already exists.' % (i, len(files), txt_basename, ))\n",
    "    continue\n",
    "\n",
    "  pdf_path = os.path.join(pdf_dir, f)\n",
    "  txt_path = os.path.join(txt_dir, txt_basename)\n",
    "  \n",
    "  cmd = \"pdftotext %s %s\" % (pdf_path, txt_path)\n",
    "  os.system(cmd)\n",
    "\n",
    "  print('%d/%d %s' % (i, len(files), cmd))\n",
    "\n",
    "  # check output was made\n",
    "  if not os.path.isfile(txt_path):\n",
    "    # there was an error with converting the pdf\n",
    "    print('there was a problem with parsing %s to text, creating an empty text file.' % (pdf_path, ))\n",
    "    os.system('touch ' + txt_path) # create empty file, but it's a record of having tried to convert\n",
    "\n",
    "  time.sleep(0.01) # allos for ctrl+c termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be nice I should parse these a bit and clean them up\n",
    "# THEN open txt files and insert into DF (naturally create DF)\n",
    "\n",
    "# See txts_to_csv notebook, easy task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response():\n",
    "    #base = test_link\n",
    "    final_url = base_url\n",
    "    #driver = webdriver.Chrome()\n",
    "    \n",
    "    with requests.get(final_url) as response:\n",
    "        #response = requests.get(url, headers=headers)\n",
    "        page_soup = soup(response.content, 'lxml')\n",
    "        return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seleniumwire import webdriver  # Import from seleniumwire\n",
    "\n",
    "# Create a new instance of the Firefox driver\n",
    "#driver = webdriver.Chrome()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--user-agent=\"Mozilla/5.0 (Windows Phone 10.0; Android 4.2.1; Microsoft; Lumia 640 XL LTE) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Mobile Safari/537.36 Edge/12.10166\"')\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "user_agent = driver.execute_script(\"return navigator.userAgent;\")\n",
    "# Go to the Google home page\n",
    "\n",
    "driver.get(base_url_1)\n",
    "\n",
    "# with driver.requests.get(base_url) as response:\n",
    "#     #response = requests.get(url, headers=headers)\n",
    "#     page_soup = soup(response.content, 'lxml')\n",
    "#     #return page_soup\n",
    "\n",
    "# Access requests via the `requests` attribute\n",
    "for request in driver.requests:\n",
    "    if request.response:\n",
    "        print(\n",
    "            request.path,\n",
    "            request.response.status_code,\n",
    "            request.response.headers['Content-Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f045682951559cbc0979d5d7223b93f289f756c5241efdcb485f4eca938569a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('dri_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
